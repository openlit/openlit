### 3. Visualize in Highlight.io

Once your LLM application is instrumented, you can explore the telemetry data in Highlight.io:

1. **Navigate to Traces**: Go to your Highlight.io project dashboard and click on **Traces**
1. **Explore AI Operations**: View your AI application traces including:
   - LLM request traces with detailed timing
   - Token usage and cost information
   - Vector database operations
   - Model performance analytics
   - Request/response payloads (if enabled)
1. **Session Monitoring**: Link traces to user sessions for full-stack observability
1. **Error Tracking**: Monitor and debug AI application errors and exceptions
1. **Performance Analysis**: Analyze latency, throughput, and resource usage

Your OpenLIT-instrumented AI applications will appear automatically in Highlight.io with comprehensive observability including LLM costs, token usage, model performance, and integration with your existing application monitoring.
