---
title: 'OpenLIT'
description: 'Complete AI observability with OpenLIT instrumentation provider'
---

The OpenLIT provider offers the most comprehensive AI observability solution, combining deep technical instrumentation with business intelligence features. Perfect for production environments requiring complete visibility into AI applications.

## ðŸš€ Overview

OpenLIT provides enterprise-grade AI observability with:

- **Complete Coverage**: 50+ LLM providers, 20+ AI frameworks, 10+ vector databases
- **Business Intelligence**: Cost tracking, performance analytics, ROI metrics
- **Production Features**: Guardrails, prompt management, team collaboration
- **Advanced Telemetry**: Custom metrics, detailed traces, performance insights

## âœ¨ Key Features

<CardGroup cols={2}>
  <Card title="ðŸ’° Cost Intelligence" icon="chart-line">
    **Token-level cost tracking**
    
    Real-time cost monitoring, budget alerts, and optimization recommendations
  </Card>
  
  <Card title="ðŸ” Deep Observability" icon="magnifying-glass">
    **Comprehensive telemetry**
    
    Traces, metrics, logs, and custom business metrics with full context
  </Card>
  
  <Card title="ðŸ›¡ï¸ Production Ready" icon="shield-check">
    **Enterprise features**
    
    Guardrails, PII redaction, compliance tools, and security controls
  </Card>
  
  <Card title="ðŸ“Š Business Analytics" icon="chart-bar">
    **Performance insights**
    
    Usage patterns, model performance, user satisfaction, and ROI analysis
  </Card>
</CardGroup>

## ðŸŽ¯ Supported Integrations

### LLM Providers (50+)

| Provider | Coverage | Cost Tracking | Streaming | Notes |
|----------|----------|---------------|-----------|-------|
| **OpenAI** | âœ… Complete | âœ… Real-time | âœ… Yes | GPT-4, GPT-3.5, Embeddings, DALL-E |
| **Anthropic** | âœ… Complete | âœ… Real-time | âœ… Yes | Claude 3, Claude 2, Claude Instant |
| **Google** | âœ… Complete | âœ… Real-time | âœ… Yes | Gemini, PaLM, Vertex AI |
| **Azure OpenAI** | âœ… Complete | âœ… Real-time | âœ… Yes | All Azure OpenAI models |
| **AWS Bedrock** | âœ… Complete | âœ… Real-time | âœ… Yes | All Bedrock models |
| **Ollama** | âœ… Complete | âœ… Estimated | âœ… Yes | Local model deployment |
| **Groq** | âœ… Complete | âœ… Real-time | âœ… Yes | High-speed inference |
| **Together AI** | âœ… Complete | âœ… Real-time | âœ… Yes | Open-source models |
| **Mistral AI** | âœ… Complete | âœ… Real-time | âœ… Yes | Mistral models |
| **Cohere** | âœ… Complete | âœ… Real-time | âœ… Yes | Command, Embed models |

### AI Frameworks (20+)

| Framework | Integration | Features | Auto-Detection |
|-----------|-------------|----------|----------------|
| **LangChain** | âœ… Native | Chains, agents, tools, memory | âœ… Yes |
| **LlamaIndex** | âœ… Native | Indexing, querying, retrieval | âœ… Yes |
| **CrewAI** | âœ… Native | Multi-agent coordination | âœ… Yes |
| **Haystack** | âœ… Native | Pipeline execution | âœ… Yes |
| **AG2 (AutoGen)** | âœ… Native | Agent conversations | âœ… Yes |
| **DSPy** | âœ… Native | Program synthesis | âœ… Yes |
| **Guardrails** | âœ… Native | Output validation | âœ… Yes |
| **Mem0** | âœ… Native | Memory management | âœ… Yes |
| **LiteLLM** | âœ… Native | Provider abstraction | âœ… Yes |
| **Pydantic AI** | âœ… Native | Structured generation | âœ… Yes |

### Vector Databases (10+)

| Database | Operations | Performance | Metadata |
|----------|------------|-------------|----------|
| **ChromaDB** | Query, insert, update, delete | âœ… Yes | âœ… Full |
| **Pinecone** | Query, upsert, delete, fetch | âœ… Yes | âœ… Full |
| **Qdrant** | Search, insert, update, delete | âœ… Yes | âœ… Full |
| **Milvus** | Query, insert, update, delete | âœ… Yes | âœ… Full |
| **Weaviate** | Query, insert, update, delete | âœ… Yes | âœ… Full |

## âš™ï¸ Configuration

### Basic Configuration

```yaml
apiVersion: openlit.io/v1alpha1
kind: AutoInstrumentation
metadata:
  name: openlit-instrumentation
spec:
  selector:
    matchLabels:
      openlit.io/instrument: "true"
  python:
    instrumentation:
      provider: "openlit"
      version: "latest"
  otlp:
    endpoint: "http://openlit:4318"
```

### Advanced Configuration

```yaml
apiVersion: openlit.io/v1alpha1
kind: AutoInstrumentation
metadata:
  name: openlit-advanced
spec:
  selector:
    matchLabels:
      provider: "openlit"
  python:
    instrumentation:
      provider: "openlit"
      version: "v1.0.0"
      customPackages: "langchain>=0.1.0,chromadb>=0.4.0,pinecone-client>=2.0.0"
      env:
      # OpenLIT-specific configuration
      - name: OPENLIT_API_KEY
        valueFrom:
          secretKeyRef:
            name: openlit-secret
            key: api-key
      - name: OPENLIT_ENVIRONMENT
        value: "production"
      - name: OPENLIT_APPLICATION_NAME
        value: "ai-customer-support"
      - name: OPENLIT_CAPTURE_MESSAGE_CONTENT
        value: "true"
      - name: OPENLIT_REDACT_PII
        value: "true"
      - name: OPENLIT_COST_TRACKING
        value: "true"
      - name: OPENLIT_METRICS_ENABLED
        value: "true"
  otlp:
    endpoint: "http://openlit:4318"
    headers: "authorization=Bearer token123"
    timeout: 60
  resource:
    environment: "production"
    serviceName: "customer-support-ai"
    serviceNamespace: "customer-ops"
```

## ðŸ”§ Environment Variables

### Core Configuration

| Variable | Description | Default | Example | Required |
|----------|-------------|---------|---------|----------|
| `OPENLIT_API_KEY` | OpenLIT API key for cloud features | `""` | `"ol-xxxxx"` | No |
| `OPENLIT_ENVIRONMENT` | Environment name | `""` | `"production"` | No |
| `OPENLIT_APPLICATION_NAME` | Application identifier | Auto-detected | `"chatbot"` | No |
| `OPENLIT_SERVICE_VERSION` | Service version | `""` | `"v1.2.0"` | No |

### Data Collection

| Variable | Description | Default | Example | Impact |
|----------|-------------|---------|---------|---------|
| `OPENLIT_CAPTURE_MESSAGE_CONTENT` | Capture prompt/response content | `true` | `"false"` | Privacy/debugging |
| `OPENLIT_REDACT_PII` | Enable PII redaction | `false` | `"true"` | Security |
| `OPENLIT_TRACE_CONTENT` | Trace message content | `true` | `"false"` | Storage size |
| `OPENLIT_CAPTURE_TOOL_CALLS` | Capture tool/function calls | `true` | `"false"` | Detail level |

### Performance

| Variable | Description | Default | Example | Impact |
|----------|-------------|---------|---------|---------|
| `OPENLIT_METRICS_ENABLED` | Enable custom metrics | `true` | `"false"` | Performance |
| `OPENLIT_BATCH_SIZE` | Telemetry batch size | `100` | `"50"` | Memory usage |
| `OPENLIT_EXPORT_TIMEOUT` | Export timeout (seconds) | `30` | `"60"` | Reliability |
| `OPENLIT_SAMPLING_RATE` | Trace sampling rate | `1.0` | `"0.1"` | Volume |

### Cost Tracking

| Variable | Description | Default | Example | Purpose |
|----------|-------------|---------|---------|---------|
| `OPENLIT_COST_TRACKING` | Enable cost calculation | `true` | `"false"` | Budget monitoring |
| `OPENLIT_COST_CURRENCY` | Currency for cost display | `"USD"` | `"EUR"` | Localization |
| `OPENLIT_CUSTOM_PRICING` | Custom pricing configuration | `""` | JSON config | Custom models |

## ðŸ“Š Telemetry Data

### Trace Spans

OpenLIT creates detailed trace spans for all AI operations:

```
ðŸ” HTTP Request
â”œâ”€â”€ ðŸ¤– llm.chat.completions (OpenAI GPT-4)
â”‚   â”œâ”€â”€ ðŸ“ prompt.preprocessing
â”‚   â”œâ”€â”€ ðŸŒ http.request (API call)
â”‚   â”œâ”€â”€ ðŸ’° cost.calculation
â”‚   â””â”€â”€ ðŸ“„ response.processing
â”œâ”€â”€ ðŸ—„ï¸ vectordb.query (ChromaDB)
â”‚   â”œâ”€â”€ ðŸ” embedding.create
â”‚   â”œâ”€â”€ ðŸ” similarity.search
â”‚   â””â”€â”€ ðŸ“Š result.ranking
â””â”€â”€ ðŸ§  langchain.chain.execute
    â”œâ”€â”€ ðŸ“‹ chain.preparation
    â”œâ”€â”€ ðŸ”§ tool.execution
    â””â”€â”€ ðŸ”„ memory.update
```

### Span Attributes

| Attribute | Description | Example | Use Case |
|-----------|-------------|---------|----------|
| `gen_ai.operation.name` | AI operation type | `"chat.completions"` | Operation filtering |
| `gen_ai.request.model` | Model name | `"gpt-4-0125-preview"` | Model analysis |
| `gen_ai.usage.input_tokens` | Input token count | `150` | Cost calculation |
| `gen_ai.usage.output_tokens` | Output token count | `75` | Cost calculation |
| `gen_ai.request.max_tokens` | Max tokens limit | `1000` | Configuration tracking |
| `gen_ai.request.temperature` | Model temperature | `0.7` | Parameter tracking |
| `openlit.cost.total` | Total operation cost | `0.0045` | Budget monitoring |
| `openlit.cost.currency` | Cost currency | `"USD"` | Financial reporting |

### Custom Metrics

OpenLIT automatically creates custom metrics:

| Metric | Type | Description | Labels |
|--------|------|-------------|--------|
| `openlit_llm_requests_total` | Counter | Total LLM requests | model, provider, status |
| `openlit_llm_duration_seconds` | Histogram | Request duration | model, provider |
| `openlit_llm_tokens_total` | Counter | Token usage | model, type (input/output) |
| `openlit_llm_cost_total` | Counter | Cumulative cost | model, provider, currency |
| `openlit_vectordb_operations_total` | Counter | Vector DB operations | database, operation |
| `openlit_framework_operations_total` | Counter | Framework operations | framework, operation |

## ðŸ’¡ Advanced Features

### Cost Optimization

OpenLIT provides advanced cost optimization features:

```yaml
env:
- name: OPENLIT_COST_TRACKING
  value: "true"
- name: OPENLIT_COST_ALERTS
  value: "true"
- name: OPENLIT_BUDGET_THRESHOLD
  value: "100.0"  # Daily budget in USD
- name: OPENLIT_COST_OPTIMIZATION
  value: "true"   # Enable optimization suggestions
```

**Cost features:**
- Real-time cost calculation
- Budget alerts and notifications
- Model cost comparison
- Optimization recommendations
- Historical cost analysis

### PII Redaction

Automatically redact sensitive information:

```yaml
env:
- name: OPENLIT_REDACT_PII
  value: "true"
- name: OPENLIT_PII_PATTERNS
  value: "email,phone,ssn,credit_card"
- name: OPENLIT_REDACTION_CHAR
  value: "*"
```

**PII detection:**
- Email addresses
- Phone numbers
- Social Security Numbers
- Credit card numbers
- Custom patterns (regex)

### Performance Analytics

Advanced performance monitoring:

```yaml
env:
- name: OPENLIT_PERFORMANCE_ANALYTICS
  value: "true"
- name: OPENLIT_LATENCY_PERCENTILES
  value: "50,90,95,99"
- name: OPENLIT_TRACK_USER_SATISFACTION
  value: "true"
```

**Analytics features:**
- Response time analysis
- User satisfaction tracking
- Model performance comparison
- A/B testing support
- Usage pattern analysis

## ðŸ”§ Framework-Specific Configurations

### LangChain Applications

```yaml
spec:
  python:
    instrumentation:
      provider: "openlit"
      customPackages: "langchain>=0.1.0,langchain-community>=0.0.20,langchain-openai>=0.0.5"
      env:
      - name: OPENLIT_TRACE_LANGCHAIN_CHAINS
        value: "true"
      - name: OPENLIT_TRACE_LANGCHAIN_AGENTS
        value: "true"
      - name: OPENLIT_TRACE_LANGCHAIN_TOOLS
        value: "true"
      - name: OPENLIT_CAPTURE_CHAIN_INPUTS
        value: "true"
      - name: OPENLIT_CAPTURE_CHAIN_OUTPUTS
        value: "true"
```

### LlamaIndex Applications

```yaml
spec:
  python:
    instrumentation:
      provider: "openlit"
      customPackages: "llama-index>=0.9.0,llama-index-vector-stores-chroma>=0.1.0"
      env:
      - name: OPENLIT_TRACE_LLAMAINDEX_QUERIES
        value: "true"
      - name: OPENLIT_TRACE_LLAMAINDEX_RETRIEVERS
        value: "true"
      - name: OPENLIT_TRACE_LLAMAINDEX_EMBEDDINGS
        value: "true"
      - name: OPENLIT_CAPTURE_QUERY_CONTEXT
        value: "true"
```

### Multi-Agent Systems

```yaml
spec:
  python:
    instrumentation:
      provider: "openlit"
      customPackages: "crewai>=0.1.0,autogen>=0.2.0"
      env:
      - name: OPENLIT_TRACE_AGENT_CONVERSATIONS
        value: "true"
      - name: OPENLIT_TRACE_AGENT_TOOLS
        value: "true"
      - name: OPENLIT_CAPTURE_AGENT_THOUGHTS
        value: "true"
      - name: OPENLIT_TRACK_AGENT_PERFORMANCE
        value: "true"
```

## ðŸ“Š Dashboard Integration

### OpenLIT Cloud Dashboard

When using OpenLIT Cloud, configure the API key:

```yaml
env:
- name: OPENLIT_API_KEY
  valueFrom:
    secretKeyRef:
      name: openlit-cloud-secret
      key: api-key
- name: OPENLIT_CLOUD_ENDPOINT
  value: "https://cloud.openlit.io"
```

### Self-Hosted Dashboard

For self-hosted OpenLIT deployments:

```yaml
env:
- name: OPENLIT_DASHBOARD_URL
  value: "http://openlit-dashboard:3000"
- name: OPENLIT_API_ENDPOINT
  value: "http://openlit-api:8080"
```

## ðŸš¨ Troubleshooting

<AccordionGroup>
  <Accordion title="OpenLIT packages not installing">
    **Symptoms:** Init container fails with package installation errors
    
    **Solutions:**
    ```bash
    # Check package versions
    kubectl logs pod-name -c openlit-instrumentation
    
    # Use specific versions
    spec:
      python:
        instrumentation:
          customPackages: "openlit==1.0.0"
    ```
  </Accordion>
  
  <Accordion title="High memory usage">
    **Symptoms:** Applications using excessive memory
    
    **Solutions:**
    ```yaml
    env:
    - name: OPENLIT_BATCH_SIZE
      value: "50"  # Reduce batch size
    - name: OPENLIT_SAMPLING_RATE
      value: "0.1"  # Sample 10% of traces
    - name: OPENLIT_METRICS_ENABLED
      value: "false"  # Disable custom metrics
    ```
  </Accordion>
  
  <Accordion title="Cost tracking not working">
    **Symptoms:** No cost data in traces
    
    **Solutions:**
    ```yaml
    env:
    - name: OPENLIT_COST_TRACKING
      value: "true"
    - name: OPENLIT_API_KEY
      valueFrom:
        secretKeyRef:
          name: openlit-secret
          key: api-key
    ```
  </Accordion>
  
  <Accordion title="Missing trace data">
    **Symptoms:** Incomplete traces or missing spans
    
    **Solutions:**
    ```bash
    # Check instrumentation logs
    kubectl logs pod-name | grep -i openlit
    
    # Verify environment variables
    kubectl exec pod-name -- env | grep OPENLIT
    
    # Check OTLP connectivity
    kubectl exec pod-name -- curl http://openlit:4318/v1/traces
    ```
  </Accordion>
</AccordionGroup>

## ðŸ“ˆ Performance Optimization

### Resource Tuning

```yaml
# Optimize for high-throughput applications
env:
- name: OPENLIT_BATCH_SIZE
  value: "200"
- name: OPENLIT_EXPORT_TIMEOUT
  value: "60"
- name: OPENLIT_BUFFER_SIZE
  value: "1000"

# Optimize for memory-constrained environments
env:
- name: OPENLIT_BATCH_SIZE
  value: "25"
- name: OPENLIT_SAMPLING_RATE
  value: "0.1"
- name: OPENLIT_TRACE_CONTENT
  value: "false"
```

### Network Optimization

```yaml
# Reduce network overhead
env:
- name: OPENLIT_COMPRESSION
  value: "gzip"
- name: OPENLIT_BATCH_TIMEOUT
  value: "5000"  # Batch for 5 seconds
- name: OPENLIT_RETRY_ENABLED
  value: "true"
```

## ðŸ“– Next Steps

<CardGroup cols={2}>
  <Card title="ðŸ“Š OpenInference Provider" href="/latest/operator/instrumentations-openinference" icon="chart-bar">
    Learn about OpenTelemetry standard compliance
  </Card>
  <Card title="ðŸ¤– OpenLLMetry Provider" href="/latest/operator/instrumentations-openllmetry" icon="brain">
    Explore LLM-focused observability
  </Card>
  <Card title="ðŸ“Š View Traces" href="/latest/operator/view-traces" icon="chart-line">
    Analyze traces in the OpenLIT dashboard
  </Card>
  <Card title="ðŸŒ Destinations" href="/latest/operator/destinations-overview" icon="globe">
    Configure trace destinations and backends
  </Card>
</CardGroup>
