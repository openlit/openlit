---
title: 'OpenInference'
description: 'OpenTelemetry standard compliance with OpenInference instrumentation'
---

The OpenInference provider offers OpenTelemetry-standard compliant instrumentation for AI applications, ensuring maximum compatibility and vendor neutrality. Perfect for organizations prioritizing open standards and multi-backend observability.

## ğŸ“Š Overview

OpenInference provides standards-focused AI observability with:

- **OpenTelemetry Native**: 100% compliance with OpenTelemetry semantic conventions
- **Vendor Neutral**: No vendor lock-in, works with any OTLP-compatible backend
- **Broad Compatibility**: Supports 40+ LLM providers and 15+ AI frameworks
- **Standards First**: Contributes to and follows OpenTelemetry community standards

<Note>
OpenInference is an open standard for AI observability built on OpenTelemetry. Learn more at [OpenInference Documentation](https://openinference.io/).
</Note>

## âœ¨ Key Features

<CardGroup cols={2}>
  <Card title="ğŸ”„ Standard Compliance" icon="check-circle">
    **100% OpenTelemetry compatible**
    
    Full adherence to OpenTelemetry semantic conventions and standards
  </Card>
  
  <Card title="ğŸŒ Vendor Neutral" icon="globe">
    **No vendor lock-in**
    
    Send data to any OpenTelemetry-compatible backend or multiple backends
  </Card>
  
  <Card title="ğŸ”§ Flexible Backend" icon="settings">
    **Multi-backend support**
    
    Simultaneously send to Jaeger, Grafana, DataDog, New Relic, and more
  </Card>
  
  <Card title="ğŸ¯ AI-Focused" icon="brain">
    **Semantic conventions**
    
    Purpose-built semantic conventions for LLMs, embeddings, and retrieval
  </Card>
</CardGroup>

## ğŸ¯ Supported Integrations

### LLM Providers (40+)

| Provider | Coverage | Semantic Conventions | Streaming | Notes |
|----------|----------|---------------------|-----------|-------|
| **OpenAI** | âœ… Complete | âœ… Standard | âœ… Yes | GPT models, embeddings, images |
| **Anthropic** | âœ… Complete | âœ… Standard | âœ… Yes | Claude family models |
| **Google** | âœ… Complete | âœ… Standard | âœ… Yes | Gemini, PaLM, Vertex AI |
| **Azure OpenAI** | âœ… Complete | âœ… Standard | âœ… Yes | All Azure OpenAI services |
| **AWS Bedrock** | âœ… Complete | âœ… Standard | âœ… Yes | All Bedrock models |
| **Ollama** | âœ… Complete | âœ… Standard | âœ… Yes | Local model inference |
| **Cohere** | âœ… Complete | âœ… Standard | âœ… Yes | Command and Embed models |
| **Mistral AI** | âœ… Complete | âœ… Standard | âœ… Yes | Mistral model family |
| **Together AI** | âœ… Complete | âœ… Standard | âœ… Yes | Open-source model hosting |
| **Groq** | âœ… Complete | âœ… Standard | âœ… Yes | High-speed inference |

### AI Frameworks (15+)

| Framework | Integration | Semantic Conventions | Auto-Detection |
|-----------|-------------|---------------------|----------------|
| **LangChain** | âœ… Native | âœ… Standard | âœ… Yes |
| **LlamaIndex** | âœ… Native | âœ… Standard | âœ… Yes |
| **Haystack** | âœ… Native | âœ… Standard | âœ… Yes |
| **DSPy** | âœ… Native | âœ… Standard | âœ… Yes |
| **LiteLLM** | âœ… Native | âœ… Standard | âœ… Yes |
| **Guardrails** | âœ… Native | âœ… Standard | âœ… Yes |
| **AutoGen** | âœ… Native | âœ… Standard | âœ… Yes |
| **CrewAI** | âœ… Native | âœ… Standard | âœ… Yes |

### Vector Databases (8+)

| Database | Operations | Standard Attributes | Performance Metrics |
|----------|------------|-------------------|-------------------|
| **ChromaDB** | Query, insert, update, delete | âœ… Yes | âœ… Yes |
| **Pinecone** | Query, upsert, delete, fetch | âœ… Yes | âœ… Yes |
| **Qdrant** | Search, insert, update, delete | âœ… Yes | âœ… Yes |
| **Weaviate** | Query, insert, update, delete | âœ… Yes | âœ… Yes |
| **Milvus** | Query, insert, update, delete | âœ… Yes | âœ… Yes |

## âš™ï¸ Configuration

### Basic Configuration

```yaml
apiVersion: openlit.io/v1alpha1
kind: AutoInstrumentation
metadata:
  name: openinference-instrumentation
spec:
  selector:
    matchLabels:
      openlit.io/instrument: "true"
  python:
    instrumentation:
      provider: "openinference"
      version: "latest"
  otlp:
    endpoint: "http://otel-collector:4318"
```

### Advanced Configuration

```yaml
apiVersion: openlit.io/v1alpha1
kind: AutoInstrumentation
metadata:
  name: openinference-advanced
spec:
  selector:
    matchLabels:
      provider: "openinference"
  python:
    instrumentation:
      provider: "openinference"
      version: "v0.5.0"
      customPackages: "langchain>=0.1.0,llama-index>=0.9.0,openinference-instrumentation[all]>=0.1.0"
      env:
      # OpenTelemetry standard configuration
      - name: OTEL_SERVICE_NAME
        value: "ai-application"
      - name: OTEL_SERVICE_VERSION
        value: "v1.0.0"
      - name: OTEL_RESOURCE_ATTRIBUTES
        value: "deployment.environment=production,team=ai-platform"
      # OpenInference-specific configuration
      - name: OPENINFERENCE_HIDE_INPUTS
        value: "false"
      - name: OPENINFERENCE_HIDE_OUTPUTS
        value: "false"
      - name: OPENINFERENCE_HIDE_INPUT_MESSAGES
        value: "false"
      - name: OPENINFERENCE_HIDE_OUTPUT_MESSAGES
        value: "false"
      - name: OPENINFERENCE_BASE64_IMAGE_MAX_LENGTH
        value: "1000"
  otlp:
    endpoint: "http://otel-collector:4318"
    headers: "x-api-key=your-api-key"
    timeout: 30
  resource:
    environment: "production"
    serviceName: "intelligent-search"
    serviceNamespace: "search-platform"
```

## ğŸ”§ Environment Variables

### Core OpenTelemetry Configuration

| Variable | Description | Default | Example | Required |
|----------|-------------|---------|---------|----------|
| `OTEL_SERVICE_NAME` | Service name | Auto-detected | `"ai-chatbot"` | No |
| `OTEL_SERVICE_VERSION` | Service version | `""` | `"v2.1.0"` | No |
| `OTEL_RESOURCE_ATTRIBUTES` | Resource attributes | `""` | `"env=prod,team=ai"` | No |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | OTLP endpoint | Auto-configured | `"http://jaeger:4318"` | No |

### OpenInference-Specific Configuration

| Variable | Description | Default | Example | Privacy Impact |
|----------|-------------|---------|---------|----------------|
| `OPENINFERENCE_HIDE_INPUTS` | Hide LLM inputs | `false` | `"true"` | High |
| `OPENINFERENCE_HIDE_OUTPUTS` | Hide LLM outputs | `false` | `"true"` | High |
| `OPENINFERENCE_HIDE_INPUT_MESSAGES` | Hide input messages | `false` | `"true"` | Medium |
| `OPENINFERENCE_HIDE_OUTPUT_MESSAGES` | Hide output messages | `false` | `"true"` | Medium |
| `OPENINFERENCE_HIDE_EMBEDDINGS` | Hide embedding vectors | `true` | `"false"` | Low |

### Content and Privacy Controls

| Variable | Description | Default | Example | Use Case |
|----------|-------------|---------|---------|----------|
| `OPENINFERENCE_BASE64_IMAGE_MAX_LENGTH` | Max base64 image length | `1000` | `"5000"` | Image handling |
| `OPENINFERENCE_HIDE_EMBEDDING_VECTORS` | Hide embedding data | `true` | `"false"` | Vector analysis |
| `OPENINFERENCE_MASK_CREDENTIALS` | Mask API credentials | `true` | `"false"` | Security |

## ğŸ“Š Semantic Conventions

OpenInference follows OpenTelemetry semantic conventions with AI-specific extensions:

### LLM Span Attributes

| Attribute | Description | Example | Standard |
|-----------|-------------|---------|----------|
| `llm.invocation_parameters` | Model parameters | `{"temperature": 0.7}` | OpenInference |
| `llm.model_name` | Model identifier | `"gpt-4-0125-preview"` | OpenInference |
| `llm.provider` | LLM provider | `"openai"` | OpenInference |
| `llm.system_message` | System prompt | `"You are a helpful assistant"` | OpenInference |
| `llm.input_messages` | Input message array | `[{"role": "user", "content": "Hello"}]` | OpenInference |
| `llm.output_messages` | Output message array | `[{"role": "assistant", "content": "Hi!"}]` | OpenInference |
| `llm.token_count.prompt` | Input token count | `25` | OpenInference |
| `llm.token_count.completion` | Output token count | `15` | OpenInference |

### Embedding Span Attributes

| Attribute | Description | Example | Standard |
|-----------|-------------|---------|----------|
| `embedding.model_name` | Embedding model | `"text-embedding-ada-002"` | OpenInference |
| `embedding.vector` | Embedding vector | `[0.1, 0.2, ...]` | OpenInference |
| `embedding.text` | Input text | `"Document content"` | OpenInference |

### Retrieval Span Attributes

| Attribute | Description | Example | Standard |
|-----------|-------------|---------|----------|
| `retrieval.documents` | Retrieved documents | `[{"id": "doc1", "score": 0.9}]` | OpenInference |
| `retrieval.query` | Search query | `"AI observability"` | OpenInference |

## ğŸŒ Multi-Backend Configuration

### Simultaneous Export to Multiple Backends

```yaml
spec:
  python:
    instrumentation:
      provider: "openinference"
      env:
      # Primary backend: Jaeger
      - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
        value: "http://jaeger-collector:4318/v1/traces"
      # Secondary backend: Configure via collector
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: "http://otel-collector:4318"
```

### OpenTelemetry Collector Configuration

Configure the collector to fan out to multiple backends:

```yaml
# otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

exporters:
  # Jaeger
  jaeger:
    endpoint: jaeger-collector:14250
    tls:
      insecure: true
  
  # Grafana Tempo
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true
  
  # DataDog
  datadog:
    api:
      key: "${DD_API_KEY}"
      site: datadoghq.com

service:
  pipelines:
    traces:
      receivers: [otlp]
      exporters: [jaeger, otlp/tempo, datadog]
```

## ğŸ”§ Framework-Specific Configurations

### LangChain with OpenInference

```yaml
spec:
  python:
    instrumentation:
      provider: "openinference"
      customPackages: "langchain>=0.1.0,openinference-instrumentation-langchain>=0.1.0"
      env:
      - name: LANGCHAIN_TRACING_V2
        value: "false"  # Disable LangSmith if using OpenInference
      - name: OPENINFERENCE_HIDE_INPUTS
        value: "false"
      - name: OPENINFERENCE_HIDE_OUTPUTS
        value: "false"
```

### LlamaIndex with OpenInference

```yaml
spec:
  python:
    instrumentation:
      provider: "openinference"
      customPackages: "llama-index>=0.9.0,openinference-instrumentation-llama-index>=0.1.0"
      env:
      - name: OPENINFERENCE_HIDE_EMBEDDING_VECTORS
        value: "true"  # Reduce payload size
      - name: OPENINFERENCE_HIDE_INPUT_MESSAGES
        value: "false"
```

### DSPy with OpenInference

```yaml
spec:
  python:
    instrumentation:
      provider: "openinference"
      customPackages: "dspy>=2.0.0,openinference-instrumentation-dspy>=0.1.0"
      env:
      - name: OPENINFERENCE_HIDE_INPUTS
        value: "false"
      - name: OPENINFERENCE_HIDE_OUTPUTS
        value: "false"
```

## ğŸ“Š Telemetry Data Structure

### Trace Example

OpenInference creates standardized trace structures:

```
ğŸ” HTTP Request (gen_ai.operation.name: "chat")
â”œâ”€â”€ ğŸ¤– LLM Call (llm.provider: "openai")
â”‚   â”œâ”€â”€ Attributes:
â”‚   â”‚   â”œâ”€â”€ llm.model_name: "gpt-4-0125-preview"
â”‚   â”‚   â”œâ”€â”€ llm.token_count.prompt: 150
â”‚   â”‚   â”œâ”€â”€ llm.token_count.completion: 75
â”‚   â”‚   â””â”€â”€ llm.invocation_parameters: {"temperature": 0.7}
â”‚   â””â”€â”€ Events:
â”‚       â”œâ”€â”€ llm.content.prompt
â”‚       â””â”€â”€ llm.content.completion
â”œâ”€â”€ ğŸ—„ï¸ Vector Search (db.operation.name: "search")
â”‚   â”œâ”€â”€ Attributes:
â”‚   â”‚   â”œâ”€â”€ db.system: "chroma"
â”‚   â”‚   â”œâ”€â”€ retrieval.query: "AI observability"
â”‚   â”‚   â””â”€â”€ retrieval.limit: 10
â”‚   â””â”€â”€ Events:
â”‚       â””â”€â”€ retrieval.documents
â””â”€â”€ ğŸ§  Framework Operation
    â”œâ”€â”€ Attributes specific to framework
    â””â”€â”€ Related events and metadata
```

### Standard Span Names

| Operation | Span Name Pattern | Example |
|-----------|------------------|---------|
| LLM Chat | `{provider}.chat` | `openai.chat` |
| LLM Completion | `{provider}.completion` | `anthropic.completion` |
| Embedding | `{provider}.embedding` | `openai.embedding` |
| Vector Search | `{db_system}.search` | `chroma.search` |
| Framework Chain | `{framework}.{operation}` | `langchain.chain` |

## ğŸ”„ Backend Integration Examples

### Jaeger Integration

```yaml
# Direct Jaeger export
env:
- name: OTEL_EXPORTER_JAEGER_ENDPOINT
  value: "http://jaeger-collector:14250"
- name: OTEL_TRACES_EXPORTER
  value: "jaeger"
```

### Grafana Tempo Integration

```yaml
# Tempo via OTLP
env:
- name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
  value: "http://tempo:4317"
- name: OTEL_EXPORTER_OTLP_HEADERS
  value: "x-scope-orgid=tenant1"
```

### DataDog Integration

```yaml
# DataDog via OTLP
env:
- name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
  value: "https://trace.agent.datadoghq.com"
- name: OTEL_EXPORTER_OTLP_HEADERS
  value: "dd-api-key=${DD_API_KEY}"
```

### New Relic Integration

```yaml
# New Relic OTLP endpoint
env:
- name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
  value: "https://otlp.nr-data.net:4318/v1/traces"
- name: OTEL_EXPORTER_OTLP_HEADERS
  value: "api-key=${NEW_RELIC_LICENSE_KEY}"
```

## ğŸš¨ Troubleshooting

<AccordionGroup>
  <Accordion title="OpenInference packages not found">
    **Symptoms:** Import errors for OpenInference instrumentation packages
    
    **Solutions:**
    ```yaml
    # Install all OpenInference packages
    spec:
      python:
        instrumentation:
          customPackages: "openinference-instrumentation[all]>=0.1.0"
    
    # Or install specific packages
    spec:
      python:
        instrumentation:
          customPackages: "openinference-instrumentation-langchain>=0.1.0,openinference-instrumentation-openai>=0.1.0"
    ```
  </Accordion>
  
  <Accordion title="Traces not appearing in backend">
    **Symptoms:** No traces visible in Jaeger, Grafana, or other backends
    
    **Solutions:**
    ```bash
    # Check OTLP endpoint connectivity
    kubectl exec pod-name -- curl http://otel-collector:4318/v1/traces
    
    # Verify OpenTelemetry configuration
    kubectl exec pod-name -- env | grep OTEL
    
    # Check OpenInference instrumentation
    kubectl logs pod-name | grep -i openinference
    ```
  </Accordion>
  
  <Accordion title="Missing AI-specific attributes">
    **Symptoms:** Standard OpenTelemetry spans but no LLM/AI attributes
    
    **Solutions:**
    ```yaml
    # Ensure AI packages are installed
    spec:
      python:
        instrumentation:
          customPackages: "langchain>=0.1.0,openai>=1.0.0"
    
    # Enable content capture
    env:
    - name: OPENINFERENCE_HIDE_INPUTS
      value: "false"
    - name: OPENINFERENCE_HIDE_OUTPUTS
      value: "false"
    ```
  </Accordion>
  
  <Accordion title="High trace volume">
    **Symptoms:** Too many traces causing performance issues
    
    **Solutions:**
    ```yaml
    # Configure sampling
    env:
    - name: OTEL_TRACES_SAMPLER
      value: "traceidratio"
    - name: OTEL_TRACES_SAMPLER_ARG
      value: "0.1"  # Sample 10%
    
    # Hide large payloads
    - name: OPENINFERENCE_HIDE_EMBEDDING_VECTORS
      value: "true"
    - name: OPENINFERENCE_BASE64_IMAGE_MAX_LENGTH
      value: "100"
    ```
  </Accordion>
</AccordionGroup>

## ğŸ“ˆ Performance Optimization

### Reduce Trace Payload Size

```yaml
env:
# Hide large content
- name: OPENINFERENCE_HIDE_EMBEDDING_VECTORS
  value: "true"
- name: OPENINFERENCE_BASE64_IMAGE_MAX_LENGTH
  value: "500"

# Use sampling
- name: OTEL_TRACES_SAMPLER
  value: "traceidratio"
- name: OTEL_TRACES_SAMPLER_ARG
  value: "0.1"

# Batch configuration
- name: OTEL_BSP_MAX_EXPORT_BATCH_SIZE
  value: "512"
- name: OTEL_BSP_EXPORT_TIMEOUT
  value: "30000"
```

### Optimize for High Throughput

```yaml
env:
# Increase batch sizes
- name: OTEL_BSP_MAX_EXPORT_BATCH_SIZE
  value: "2048"
- name: OTEL_BSP_MAX_QUEUE_SIZE
  value: "8192"

# Reduce export frequency
- name: OTEL_BSP_SCHEDULE_DELAY
  value: "5000"  # 5 seconds

# Use compression
- name: OTEL_EXPORTER_OTLP_COMPRESSION
  value: "gzip"
```

## ğŸ¢ Enterprise Considerations

### Compliance and Standards

OpenInference is ideal for organizations with:

- **Regulatory requirements** for open standards
- **Multi-vendor strategies** to avoid lock-in
- **Interoperability needs** across different tools
- **Long-term portability** requirements

### Governance Benefits

- **Vendor neutrality**: No dependency on specific vendors
- **Standard compliance**: Follows OpenTelemetry conventions
- **Community driven**: Open source with broad community support
- **Future proof**: Standards-based approach ensures longevity

## ğŸ“– Next Steps

<CardGroup cols={2}>
  <Card title="ğŸš€ OpenLIT Provider" href="/latest/operator/instrumentations-openlit" icon="rocket">
    Compare with full-featured OpenLIT instrumentation
  </Card>
  <Card title="ğŸ¤– OpenLLMetry Provider" href="/latest/operator/instrumentations-openllmetry" icon="brain">
    Explore LLM-focused observability
  </Card>
  <Card title="ğŸŒ Destinations" href="/latest/operator/destinations-overview" icon="globe">
    Configure backends and trace destinations
  </Card>
  <Card title="ğŸ”§ Custom Provider" href="/latest/operator/instrumentations-custom" icon="gear">
    Build custom instrumentation solutions
  </Card>
</CardGroup>
