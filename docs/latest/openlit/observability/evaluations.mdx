---
title: "Evaluations"
description: "Automatically evaluate AI responses for quality, safety, and performance in OpenLIT"
---

OpenLIT provides automated evaluation capabilities to continuously assess your AI applications for quality, safety, and performance. 
Auto evaluations run automatically on your LLM responses, providing real-time insights into response quality, hallucination detection, bias analysis, and safety compliance.

## Overview

Auto evaluations in OpenLIT help you:

- **Monitor Response Quality**: Automatically assess relevance, coherence, and accuracy of AI responses
- **Detect Hallucinations**: Identify when models generate false or fabricated information
- **Analyze Bias**: Monitor for potential biases in AI responses across different contexts
- **Ensure Safety**: Check responses for toxicity, harmful content, and policy violations
- **Track Performance**: Monitor evaluation metrics over time with automated scoring

## Evaluation Types

### Quality Evaluations

**Response Relevance**
- Measures how well responses answer the given prompts
- Scores based on semantic similarity and contextual appropriateness
- Helps identify when models drift off-topic or provide irrelevant information

**Coherence Analysis**
- Evaluates logical flow and consistency within responses
- Detects contradictions and inconsistent statements
- Ensures responses maintain logical structure

**Factual Accuracy**
- Cross-references responses against known facts and data sources
- Identifies potential misinformation or incorrect statements
- Provides confidence scores for factual claims

### Safety Evaluations

**Toxicity Detection**
- Automatically screens responses for harmful, offensive, or inappropriate content
- Uses multiple detection models to ensure comprehensive coverage
- Provides severity scores and content classification

**Bias Analysis**
- Monitors for gender, racial, cultural, and other forms of bias
- Analyzes response patterns across different user demographics
- Identifies potential discriminatory language or recommendations

**Policy Compliance**
- Checks responses against custom organizational policies
- Ensures adherence to content guidelines and standards
- Flags potential policy violations for review

### Performance Evaluations

**Response Time Analysis**
- Monitors latency and response generation speed
- Identifies performance degradation patterns
- Tracks efficiency metrics across different models and configurations

**Cost Effectiveness**
- Evaluates cost per response quality score
- Identifies opportunities for model optimization
- Tracks ROI of different AI configurations

## Configuration

### Enabling Auto Evaluations

Auto evaluations can be configured at the application level:

```python
import openlit

openlit.init(
    auto_evaluations=True,
    evaluation_config={
        "quality_checks": ["relevance", "coherence", "accuracy"],
        "safety_checks": ["toxicity", "bias", "policy"],
        "performance_checks": ["latency", "cost"]
    }
)
```

### Custom Evaluation Rules

Define custom evaluation criteria specific to your use case:

```python
evaluation_config = {
    "custom_rules": [
        {
            "name": "domain_accuracy",
            "type": "quality",
            "criteria": "Response must be accurate for medical domain",
            "threshold": 0.8
        },
        {
            "name": "brand_compliance",
            "type": "policy", 
            "criteria": "Response must align with brand guidelines",
            "threshold": 0.9
        }
    ]
}
```

## Viewing Evaluation Results

### Dashboard Widgets

Create evaluation-focused widgets in your OpenLIT dashboards:

**Evaluation Score Trends**
- Track evaluation scores over time
- Compare performance across different models
- Identify trends and patterns in AI quality

**Safety Compliance Metrics**
- Monitor safety violation rates
- Track improvement in safety scores
- Alert on safety threshold breaches

**Quality Distribution Charts**
- Visualize distribution of quality scores
- Identify outliers and edge cases
- Monitor consistency in response quality

### Evaluation Alerts

Set up automated alerts for evaluation thresholds:

- **Quality Degradation**: Alert when response quality drops below acceptable levels
- **Safety Violations**: Immediate notifications for policy or safety violations
- **Performance Issues**: Alerts for response time or cost threshold breaches
- **Bias Detection**: Notifications when bias scores exceed configured limits

## Best Practices

### Evaluation Strategy

**Comprehensive Coverage**
- Enable multiple evaluation types for complete assessment
- Balance automated and human evaluation approaches
- Regular review and updating of evaluation criteria

**Threshold Management**
- Set realistic thresholds based on your application requirements
- Implement gradual threshold adjustments as models improve
- Consider context-specific thresholds for different use cases

**Continuous Monitoring**
- Monitor evaluation trends continuously
- Set up alerts for significant changes in evaluation patterns
- Regular analysis of evaluation results for insights

### Integration with Development Workflow

**Pre-deployment Testing**
- Use auto evaluations in staging environments
- Validate model changes before production deployment
- Establish evaluation benchmarks for new models

**Production Monitoring**
- Continuous evaluation of live traffic
- Real-time alerting for quality or safety issues
- Historical analysis for model performance trends

## Advanced Features

### Custom Evaluators

Implement domain-specific evaluation logic:

```python
def custom_medical_evaluator(prompt, response, metadata):
    # Custom evaluation logic for medical domain
    accuracy_score = check_medical_accuracy(response)
    safety_score = check_medical_safety(response)
    
    return {
        "medical_accuracy": accuracy_score,
        "medical_safety": safety_score,
        "overall_score": (accuracy_score + safety_score) / 2
    }

openlit.register_evaluator("medical_domain", custom_medical_evaluator)
```

### Batch Evaluations

Run evaluations on historical data:

- Evaluate past conversations for quality assessment
- Benchmark new evaluation criteria against existing data
- Identify patterns and trends in historical responses

### A/B Testing Integration

Compare evaluation results across different model configurations:

- Automatic evaluation comparison between model variants
- Statistical significance testing for evaluation metrics
- Performance and quality trade-off analysis

---

<CardGroup cols={2}>
  <Card title="Programmatic Evaluations" href="/latest/sdk/quickstart-programmatic-evals" icon='code'>
    Implement custom evaluations in your code with SDK integration
  </Card>
  <Card title="Metrics" href="/latest/openlit/observability/metrics" icon='chart-line'>
    View evaluation metrics through dashboard widgets
  </Card>
</CardGroup>