---
title: "Overview"
description: "Automated Eval scoring, hallucination detection, bias monitoring for LLMs apps"
---


OpenLIT provides automated evaluation that helps you assess and monitor the quality, safety, and performance of your LLM outputs across development and production environments.

## Why Evaluations?

Evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing user experience and trust in your AI applications. Here are the key benefits:

- **Quality & Safety Assurance**: Detect hallucinations, bias, toxicity, and ensure consistent, reliable AI outputs
- **Performance Monitoring**: Track model performance degradation and measure response quality across different scenarios
- **Risk Mitigation**: Catch potential issues before they reach users and ensure compliance with safety standards
- **Cost Optimization**: Monitor cost-effectiveness and ROI of different AI configurations and model choices
- **Continuous Improvement**: Build data-driven insights for A/B testing, optimization, and iterative development

## AI Evaluation Methods

OpenLIT provides automated LLM evaluation and testing capabilities for production AI applications:

### Automated LLM-as-a-Judge

Zero-setup AI quality monitoring that automatically evaluates your LLM responses:

- **Production Monitoring**: Auto-evaluate every LLM response for quality and safety issues
- **Smart Scheduling**: Configure evaluation frequency and sampling for cost optimization
- **Real-time Scoring**: Instant evaluation results visible in trace details and dashboards

### SDK-Based Evaluations  

Programmatic AI evaluation tools for custom testing workflows and development pipelines:

- **Code Integration**: Call LLM evaluators directly in your application code
- **CI/CD Quality Gates**: Automated testing for model improvements and regression detection

---

<CardGroup cols={2}>
  <Card title="LLM-as-a-Judge" href="/latest/openlit/observability/evaluations/evaluation-methods/llm-as-a-judge" icon='gavel'>
    Use advanced LLMs to evaluate AI application quality with automated scoring
  </Card>
  <Card title="SDK Evaluations" href="/latest/sdk/features/evaluations" icon='code'>
    Implement evaluations programmatically with hallucination, bias, and toxicity detection
  </Card>
  <Card title="Programmatic Evaluations" href="/latest/sdk/quickstart-programmatic-evals" icon='bolt'>
    Quick start guide for implementing custom evaluations in your code
  </Card>
  <Card title="Metrics Dashboard" href="/latest/openlit/observability/metrics" icon='chart-line'>
    View evaluation metrics and trends through comprehensive dashboards
  </Card>
</CardGroup>