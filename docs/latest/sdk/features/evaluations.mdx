---
title: 'Evaluations'
description: 'Evaluate your model responses for Hallucination, Bias, and Toxicity'
---

## Overview

The OpenLIT SDK provides tools to evaluate AI-generated text, ensuring it aligns with desired standards and identifying issues like hallucinations, bias, and toxicity. We offer four main evaluation modules:

<CardGroup cols={2}>
  <Card title="All Evaluations" href="#all-evaluations">
    Detects and evaluates all text risks including bias, toxicity, and hallucination.
  </Card>
  <Card title="Hallucination" href="#hallucination-detection">
    Detects factual inaccuracies and contradictions in text.
  </Card>
  <Card title="Toxicity" href="#toxicity-detection">
    Detects and flags harmful or offensive language.
  </Card>
  <Card title="Bias" href="#bias-detection">
    Detects and addresses prejudiced or biased language.
  </Card>
</CardGroup>

## Evaluations

### Hallucination Detection

Evaluates text for inaccuracies compared to the given context or factual information, identifying instances where the generated content diverges from the truth.

#### Usage

Use LLM-based detection with providers like OpenAI or Anthropic:

```python
import openlit

# Optionally, set your API key as an environment variable
import os
os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>"

# Initialize the hallucination detector
hallucination_detector = openlit.evals.Hallucination(provider="openai")

# Measure hallucination in text
result = hallucination_detector.measure(
    prompt="Discuss Einstein's achievements",
    contexts=["Einstein discovered the photoelectric effect."],
    text="Einstein won the Nobel Prize in 1969 for the theory of relativity."
)
```

#### Supported Providers and LLMs
<AccordionGroup>
  <Accordion title="OpenAI">
    GPT-4o, GPT-4o mini
  </Accordion>

  <Accordion title="Anthropic">
    Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus
  </Accordion>
</AccordionGroup>

#### Parameters
<AccordionGroup>
<Accordion title="`openlit.evals.Hallucination()` Class Parameters">

These parameters are used to set up the `Hallucination` class:

| Name                | Description                                                                                       | Default Value | Example Value           |
|---------------------|---------------------------------------------------------------------------------------------------|---------------|-------------------------|
| `provider`          | The name of the LLM provider, either `"openai"` or `"anthropic"`.                                 | `"openai"`    | `"openai"`              |
| `api_key`           | API key for LLM authentication, set via `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variables. | `None`        | `os.getenv("OPENAI_API_KEY")` |
| `model`             | Specific model to use with the LLM provider (optional).                                           | `None`        | `"gpt-4o"`              |
| `base_url`          | Base URL for the LLM API (optional).                                                              | `None`        | `"https://api.openai.com/v1"` |
| `custom_categories` | Additional categories for detection (optional).                                                   | `None`        | `{"custom_category": "Custom description"}` |
| `threshold_score`   | Score above which a verdict is "yes" (indicating hallucination).                                  | `0.5`         | `0.7`                   |
| `collect_metrics`   | Enable metrics collection.                                                                        | `False`       | `True`                  |

</Accordion>

<Accordion title="`measure` Method Parameters">

These parameters are passed when you call the `measure` method to analyze a specific text:

| Name      | Description                                                  | Example Value                                              |
|-----------|--------------------------------------------------------------|------------------------------------------------------------|
| `prompt`  | The prompt provided by the user.                             | `"Discuss Einstein's achievements"`                        |
| `contexts`| A list of context sentences relevant to the task.            | `["Einstein discovered the photoelectric effect."]`        |
| `text`    | The text to analyze for hallucination.                       | `"Einstein won the Nobel Prize in 1969 for the theory of relativity."` |

</Accordion>
</AccordionGroup>

#### Classification Categories

<Accordion title="Categories">

| Category               | Definition                                                                                                     |
|------------------------|----------------------------------------------------------------------------------------------------------------|
| `factual_inaccuracy`   | Incorrect facts, e.g., Context: ["Paris is the capital of France."]; Text: "Lyon is the capital."              |
| `nonsensical_response` | Irrelevant info, e.g., Context: ["Discussing music trends."]; Text: "Golf uses clubs on grass."               |
| `gibberish`            | Nonsensical text, e.g., Context: ["Discuss advanced algorithms."]; Text: "asdas asdhasudqoiwjopakcea."       |
| `contradiction`        | Conflicting info, e.g., Context: ["Einstein was born in 1879."]; Text: "Einstein was born in 1875 and 1879." |

</Accordion>

#### How it Works
<Accordion title="Explanation">
1. **Input Gathering**: Collects prompt, contexts, and text to be analyzed.

2. **Prompt Creation**: Constructs a system prompt using the specified context and categories.

3. **LLM Evaluation**: Utilizes the LLM to evaluate the alignment of the text with provided contexts and detect hallucinations.

4. **JSON Output**: Provides results with a score, evaluation ("hallucination"), classification, explanation, and verdict ("yes" or "no").
</Accordion>

#### JSON Output:
<Accordion title="Output">
The JSON object returned includes:

```json
{
  "score": "float",
  "evaluation": "hallucination",
  "classification": "CATEGORY_OF_HALLUCINATION or none",
  "explanation": "Very short one-sentence reason",
  "verdict": "yes or no"
}
```

- **Score**: Represents the level of hallucination detected.
- **Evaluation**: Specifies the type of evaluation conducted ("hallucination").
- **Classification**: Indicates the detected type of hallucination or "none" if none found.
- **Explanation**: Provides a brief reason for the classification.
- **Verdict**: "yes" if detected (score above threshold), "no" otherwise.

</Accordion>


### Bias Detection

Identifies and evaluates instances of bias in text generated by AI models. This module leverages Language Model (LLM) analysis to ensure fair and unbiased outputs.

#### Usage

Use LLM-based detection with providers like OpenAI or Anthropic:

```python
import openlit

# Optionally, set your API key as an environment variable
import os
os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>"

# Initialize the bias detector
bias_detector = openlit.evals.BiasDetector(provider="openai")

# Measure bias in text
result = bias_detector.measure(
    prompt="Discuss workplace equality.",
    contexts=["Everyone should have equal opportunity regardless of background."],
    text="Older employees tend to struggle with new technology."
)
```

#### Supported Providers and LLMs
<AccordionGroup>
  <Accordion title="OpenAI">
    GPT-4o, GPT-4o mini
  </Accordion>

  <Accordion title="Anthropic">
    Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus
  </Accordion>
</AccordionGroup>

#### Parameters
<AccordionGroup>
<Accordion title="`openlit.evals.BiasDetector()` Class Parameters">

These parameters are used to set up the `BiasDetector` class:

| Name                | Description                                                                                       | Default Value | Example Value           |
|---------------------|---------------------------------------------------------------------------------------------------|---------------|-------------------------|
| `provider`          | The name of the LLM provider, either `"openai"` or `"anthropic"`.                                 | `"openai"`    | `"openai"`              |
| `api_key`           | API key for LLM authentication, set via `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variables. | `None`        | `os.getenv("OPENAI_API_KEY")` |
| `model`             | Specific model to use with the LLM provider (optional).                                           | `None`        | `"gpt-4o"`              |
| `base_url`          | Base URL for the LLM API (optional).                                                              | `None`        | `"https://api.openai.com/v1"` |
| `custom_categories` | Additional categories for detection (optional).                                                   | `None`        | `{"custom_category": "Custom description"}` |
| `threshold_score`   | Score above which a verdict is "yes" (indicating bias).                                           | `0.5`         | `0.6`                   |
| `collect_metrics`   | Enable metrics collection.                                                                        | `False`       | `True`                  |

</Accordion>

<Accordion title="`measure` Method Parameters">

These parameters are passed when you call the `measure` method to analyze a specific text:

| Name      | Description                                               | Example Value                                         |
|-----------|-----------------------------------------------------------|-------------------------------------------------------|
| `prompt`  | The prompt provided by the user.                          | `"Discuss workplace equality."`                       |
| `contexts`| A list of context sentences relevant to the task.         | `["Everyone should have equal opportunity regardless of background."]` |
| `text`    | The text to analyze for bias.                             | `"Older employees tend to struggle with new technology."` |

</Accordion>
</AccordionGroup>

#### Classification Categories

<Accordion title="Categories">

| Category                   | Definition                                                                                               |
|----------------------------|----------------------------------------------------------------------------------------------------------|
| `sexual_orientation`       | Biases or assumptions about an individual's sexual preferences.                                          |
| `age`                      | Biases related to the age of an individual.                                                              |
| `disability`               | Biases or stereotypes concerning individuals with disabilities.                                          |
| `physical_appearance`      | Biases based on the physical look of an individual.                                                      |
| `religion`                 | Biases or prejudices connected to a person's religious beliefs.                                          |
| `pregnancy_status`         | Biases towards individuals who are pregnant or have children.                                            |
| `marital_status`           | Biases related to whether someone is single, married, divorced, etc.                                     |
| `nationality / location`   | Biases associated with an individual's country or place of origin.                                       |
| `gender`                   | Biases related to an individual's gender.                                                                |
| `ethnicity`                | Assumptions or stereotypes based on racial or ethnic background.                                         |
| `socioeconomic_status`     | Biases regarding an individual's economic and social position.                                           |

</Accordion>

#### How it Works
<Accordion title="Explanation">
1. **Input Gathering**: Collects prompt, contexts, and text to be analyzed.

2. **Prompt Creation**: Constructs a system prompt using the specified context and categories.

3. **LLM Evaluation**: Utilizes the LLM to evaluate the text for any bias present against provided contexts.

4. **JSON Output**: Provides results with a score, evaluation ("bias_detection"), classification, explanation, and verdict ("yes" or "no").
</Accordion>

#### JSON Output:
<Accordion title="Output">
The JSON object returned includes:

```json
{
  "score": "float",
  "evaluation": "bias_detection",
  "classification": "CATEGORY_OF_BIAS or none",
  "explanation": "Very short one-sentence reason",
  "verdict": "yes or no"
}
```

- **Score**: Represents the level of bias detected.
- **Evaluation**: Specifies the type of evaluation conducted ("bias_detection").
- **Classification**: Indicates the detected type of bias or "none" if none found.
- **Explanation**: Provides a brief reason for the classification.
- **Verdict**: "yes" if bias detected (score above threshold), "no" otherwise.

</Accordion>

### Toxicity Detection

Evaluates AI-generated text for harmful or offensive language, ensuring interactions are respectful and appropriate. This module uses Language Model (LLM) analysis to detect and address toxic content.

#### Usage

Use LLM-based detection with providers like OpenAI or Anthropic:

```python
import openlit

# Optionally, set your API key as an environment variable
import os
os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>"

# Initialize the toxicity detector
toxicity_detector = openlit.evals.ToxicityDetector(provider="openai")

# Measure toxicity in text
result = toxicity_detector.measure(
    prompt="Engage in a respectful discussion about global events.",
    contexts=["Conversations should remain civil and informative."],
    text="Your opinion is absurd, and only an idiot would think that."
)
```

#### Supported Providers and LLMs
<AccordionGroup>
  <Accordion title="OpenAI">
    GPT-4o, GPT-4o mini
  </Accordion>

  <Accordion title="Anthropic">
    Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus
  </Accordion>
</AccordionGroup>

#### Parameters
<AccordionGroup>
<Accordion title="`openlit.evals.ToxicityDetector()` Class Parameters">

These parameters are used to set up the `ToxicityDetector` class:

| Name                | Description                                                                                       | Default Value | Example Value           |
|---------------------|---------------------------------------------------------------------------------------------------|---------------|-------------------------|
| `provider`          | The name of the LLM provider, either `"openai"` or `"anthropic"`.                                 | `"openai"`    | `"openai"`              |
| `api_key`           | API key for LLM authentication, set via `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variables. | `None`        | `os.getenv("OPENAI_API_KEY")` |
| `model`             | Specific model to use with the LLM provider (optional).                                           | `None`        | `"gpt-4o"`              |
| `base_url`          | Base URL for the LLM API (optional).                                                              | `None`        | `"https://api.openai.com/v1"` |
| `custom_categories` | Additional categories for detection (optional).                                                   | `None`        | `{"custom_category": "Custom description"}` |
| `threshold_score`   | Score above which a verdict is "yes" (indicating toxicity).                                       | `0.5`         | `0.6`                   |
| `collect_metrics`   | Enable metrics collection.                                                                        | `False`       | `True`                  |

</Accordion>

<Accordion title="`measure` Method Parameters">

These parameters are passed when you call the `measure` method to analyze a specific text:

| Name      | Description                                               | Example Value                                      |
|-----------|-----------------------------------------------------------|----------------------------------------------------|
| `prompt`  | The prompt provided by the user.                          | `"Engage in a respectful discussion about global events."` |
| `contexts`| A list of context sentences relevant to the task.         | `["Conversations should remain civil and informative."]`   |
| `text`    | The text to analyze for toxicity.                         | `"Your opinion is absurd, and only an idiot would think that."` |

</Accordion>
</AccordionGroup>

#### Classification Categories

<Accordion title="Categories">

| Category           | Definition                                                                                             |
|--------------------|--------------------------------------------------------------------------------------------------------|
| `threat`           | Language that threatens harm or danger to others, e.g., "You'll regret saying things like that."      |
| `dismissive`       | Dismissive or belittling language, e.g., "Your opinion is worthless."                                 |
| `hate`             | Hateful or intensely negative language, e.g., "Only an idiot would think this."                      |
| `mockery`          | Mocking or sarcastic tone, e.g., "Brilliant observation! Did it take you all day to come up with that?"|
| `personal_attack`  | Language attacking a person's character or abilities, e.g., "You're clueless."                        |

</Accordion>

#### How it Works
<Accordion title="Explanation">
1. **Input Gathering**: Collects prompt, contexts, and text to be analyzed.

2. **Prompt Creation**: Constructs a system prompt using the specified context and categories.

3. **LLM Evaluation**: Utilizes the LLM to evaluate the text for any toxic language present against provided contexts.

4. **JSON Output**: Provides results with a score, evaluation ("toxicity_detection"), classification, explanation, and verdict ("yes" or "no").
</Accordion>

#### JSON Output:
<Accordion title="Output">
The JSON object returned includes:

```json
{
  "score": "float",
  "evaluation": "toxicity_detection",
  "classification": "CATEGORY_OF_TOXICITY or none",
  "explanation": "Very short one-sentence reason",
  "verdict": "yes or no"
}
```

- **Score**: Represents the level of toxicity detected.
- **Evaluation**: Specifies the type of evaluation conducted ("toxicity_detection").
- **Classification**: Indicates the detected type of toxicity or "none" if none found.
- **Explanation**: Provides a brief reason for the classification.
- **Verdict**: "yes" if toxicity detected (score above threshold), "no" otherwise.

</Accordion>

### All Evaluations

Combines the capabilities of bias, toxicity, and hallucination detection to provide a comprehensive analysis of AI-generated text. This module ensures that interactions are accurate, respectful, and free from bias.

#### Usage

Use LLM-based detection with providers like OpenAI or Anthropic:

```python
import openlit

# Optionally, set your API key as an environment variable
import os
os.environ["OPENAI_API_KEY"] = "<YOUR_API_KEY>"

# Initialize the all evaluations detector
all_evals_detector = openlit.evals.All(provider="openai")

# Measure issues in text
result = all_evals_detector.measure(
    prompt="Discuss the achievements of scientists.",
    contexts=["Einstein discovered the photoelectric effect, contributing to quantum physics."],
    text="Einstein won the Nobel Prize in 1969 for discovering black holes."
)
```

#### Supported Providers and LLMs
<AccordionGroup>
  <Accordion title="OpenAI">
    GPT-4o, GPT-4o mini
  </Accordion>

  <Accordion title="Anthropic">
    Claude 3.5 Sonnet, Claude 3.5 Haiku, Claude 3 Opus
  </Accordion>
</AccordionGroup>

#### Parameters
<AccordionGroup>
<Accordion title="`openlit.evals.All()` Class Parameters">

These parameters are used to set up the `All` class:

| Name                | Description                                                                                       | Default Value | Example Value           |
|---------------------|---------------------------------------------------------------------------------------------------|---------------|-------------------------|
| `provider`          | The name of the LLM provider, either `"openai"` or `"anthropic"`.                                 | `"openai"`    | `"openai"`              |
| `api_key`           | API key for LLM authentication, set via `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variables. | `None`        | `os.getenv("OPENAI_API_KEY")` |
| `model`             | Specific model to use with the LLM provider (optional).                                           | `None`        | `"gpt-4o"`              |
| `base_url`          | Base URL for the LLM API (optional).                                                              | `None`        | `"https://api.openai.com/v1"` |
| `custom_categories` | Additional categories for detection (optional).                                                   | `None`        | `{"custom_category": "Custom description"}` |
| `threshold_score`   | Score above which a verdict is "yes" (indicating an issue).                                       | `0.5`         | `0.6`                   |
| `collect_metrics`   | Enable metrics collection.                                                                        | `False`       | `True`                  |

</Accordion>

<Accordion title="`measure` Method Parameters">

These parameters are passed when you call the `measure` method to analyze a specific text:

| Name      | Description                                                | Example Value                                         |
|-----------|------------------------------------------------------------|-------------------------------------------------------|
| `prompt`  | The prompt provided by the user.                           | `"Discuss the achievements of scientists."`           |
| `contexts`| A list of context sentences relevant to the task.          | `["Einstein discovered the photoelectric effect."]`   |
| `text`    | The text to analyze for bias, toxicity, or hallucination.  | `"Einstein won the Nobel Prize in 1969 for discovering black holes."` |

</Accordion>
</AccordionGroup>

#### Classification Categories

<Accordion title="Bias Categories">

| Category                   | Definition                                                                                               |
|----------------------------|----------------------------------------------------------------------------------------------------------|
| `sexual_orientation`       | Involves biases or assumptions about an individual's sexual preferences.                                 |
| `age`                      | Biases related to the age of an individual.                                                              |
| `disability`               | Biases or stereotypes concerning individuals with disabilities.                                          |
| `physical_appearance`      | Biases based on the physical look of an individual.                                                      |
| `religion`                 | Biases or prejudices connected to a person's religious beliefs.                                          |
| `pregnancy_status`         | Biases towards individuals who are pregnant or have children.                                            |
| `marital_status`           | Biases related to whether someone is single, married, divorced, etc.                                     |
| `nationality / location`   | Biases associated with an individual's country or place of origin.                                       |
| `gender`                   | Biases related to an individual's gender.                                                                |
| `ethnicity`                | Assumptions or stereotypes based on racial or ethnic background.                                         |
| `socioeconomic_status`     | Biases regarding an individual's economic and social position.                                           |

</Accordion>

<Accordion title="Toxicity Categories">

| Category           | Definition                                                                                             |
|--------------------|--------------------------------------------------------------------------------------------------------|
| `threat`           | Language that threatens harm or danger to others.                                                     |
| `dismissive`       | Dismissive or belittling language.                                                                     |
| `hate`             | Hateful or intensely negative language.                                                                |
| `mockery`          | Mocking or sarcastic tone.                                                                             |
| `personal_attack`  | Language attacking a person's character or abilities.                                                  |

</Accordion>

<Accordion title="Hallucination Categories">

| Category               | Definition                                                                                                     |
|------------------------|----------------------------------------------------------------------------------------------------------------|
| `factual_inaccuracy`   | Incorrect facts.                                                                                               |
| `nonsensical_response` | Irrelevant info.                                                                                              |
| `gibberish`            | Nonsensical text.                                                                                             |
| `contradiction`        | Conflicting info.                                                                                             |

</Accordion>

#### How it Works
<Accordion title="Explanation">
1. **Input Gathering**: Collects prompt, contexts, and text to be analyzed.

2. **Prompt Creation**: Constructs a system prompt encompassing bias, toxicity, and hallucination categories.

3. **LLM Evaluation**: Utilizes the LLM to evaluate for any presence of bias, toxicity, or hallucination against provided contexts.

4. **JSON Output**: Provides results with a score, evaluation (highest risk type), classification, explanation, and verdict ("yes" or "no").
</Accordion>

#### JSON Output:
<Accordion title="Output">
The JSON object returned includes:

```json
{
  "score": "float",
  "evaluation": "evaluation_type",
  "classification": "CATEGORY or none",
  "explanation": "Very short one-sentence reason",
  "verdict": "yes or no"
}
```

- **Score**: Represents the level of detected issue.
- **Evaluation**: Indicates the type of evaluation conducted (bias, toxicity, or hallucination).
- **Classification**: Indicates the detected type or "none" if none found.
- **Explanation**: Provides a brief reason for the classification.
- **Verdict**: "yes" if detected (score above threshold), "no" otherwise.

</Accordion>

---

<CardGroup cols={3}>
  <Card title="Deploy OpenLIT" href="/latest/openlit/installation" icon='circle-down'>
    Deployment options for scalable LLM monitoring infrastructure
  </Card>
  <Card title="Online Evaluations" href="/latest/openlit/quickstart-evals" icon='bolt'>
    Get started with evaluating your LLM responses in 2 simple steps on OpenLIT
  </Card>
  <Card title="Destinations" href="/latest/sdk/destinations/overview" icon='link'>
    Send telemetry to Datadog, Grafana, New Relic, and other observability stacks
  </Card>
</CardGroup>
<Card
    title="Running in Kubernetes? Try the OpenLIT Operator"
    icon={<svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
    <title>Kubernetes</title>
    <path fill="#FFA500" d="M10.204 14.35l.007.01-.999 2.413a5.171 5.171 0 0 1-2.075-2.597l2.578-.437.004.005a.44.44 0 0 1 .484.606zm-.833-2.129a.44.44 0 0 0 .173-.756l.002-.011L7.585 9.7a5.143 5.143 0 0 0-.73 3.255l2.514-.725.002-.009zm1.145-1.98a.44.44 0 0 0 .699-.337l.01-.005.15-2.62a5.144 5.144 0 0 0-3.01 1.442l2.147 1.523.004-.002zm.76 2.75l.723.349.722-.347.18-.78-.5-.623h-.804l-.5.623.179.779zm1.5-3.095a.44.44 0 0 0 .7.336l.008.003 2.134-1.513a5.188 5.188 0 0 0-2.992-1.442l.148 2.615.002.001zm10.876 5.97l-5.773 7.181a1.6 1.6 0 0 1-1.248.594l-9.261.003a1.6 1.6 0 0 1-1.247-.596l-5.776-7.18a1.583 1.583 0 0 1-.307-1.34L2.1 5.573c.108-.47.425-.864.863-1.073L11.305.513a1.606 1.606 0 0 1 1.385 0l8.345 3.985c.438.209.755.604.863 1.073l2.062 8.955c.108.47-.005.963-.308 1.34zm-3.289-2.057c-.042-.01-.103-.026-.145-.034-.174-.033-.315-.025-.479-.038-.35-.037-.638-.067-.895-.148-.105-.04-.18-.165-.216-.216l-.201-.059a6.45 6.45 0 0 0-.105-2.332 6.465 6.465 0 0 0-.936-2.163c.052-.047.15-.133.177-.159.008-.09.001-.183.094-.282.197-.185.444-.338.743-.522.142-.084.273-.137.415-.242.032-.024.076-.062.11-.089.24-.191.295-.52.123-.736-.172-.216-.506-.236-.745-.045-.034.027-.08.062-.111.088-.134.116-.217.23-.33.35-.246.25-.45.458-.673.609-.097.056-.239.037-.303.033l-.19.135a6.545 6.545 0 0 0-4.146-2.003l-.012-.223c-.065-.062-.143-.115-.163-.25-.022-.268.015-.557.057-.905.023-.163.061-.298.068-.475.001-.04-.001-.099-.001-.142 0-.306-.224-.555-.5-.555-.275 0-.499.249-.499.555l.001.014c0 .041-.002.092 0 .128.006.177.044.312.067.475.042.348.078.637.056.906a.545.545 0 0 1-.162.258l-.012.211a6.424 6.424 0 0 0-4.166 2.003 8.373 8.373 0 0 1-.18-.128c-.09.012-.18.04-.297-.029-.223-.15-.427-.358-.673-.608-.113-.12-.195-.234-.329-.349-.03-.026-.077-.062-.111-.088a.594.594 0 0 0-.348-.132.481.481 0 0 0-.398.176c-.172.216-.117.546.123.737l.007.005.104.083c.142.105.272.159.414.242.299.185.546.338.743.522.076.082.09.226.1.288l.16.143a6.462 6.462 0 0 0-1.02 4.506l-.208.06c-.055.072-.133.184-.215.217-.257.081-.546.11-.895.147-.164.014-.305.006-.48.039-.037.007-.09.02-.133.03l-.004.002-.007.002c-.295.071-.484.342-.423.608.061.267.349.429.645.365l.007-.001.01-.003.129-.029c.17-.046.294-.113.448-.172.33-.118.604-.217.87-.256.112-.009.23.069.288.101l.217-.037a6.5 6.5 0 0 0 2.88 3.596l-.09.218c.033.084.069.199.044.282-.097.252-.263.517-.452.813-.091.136-.185.242-.268.399-.02.037-.045.095-.064.134-.128.275-.034.591.213.71.248.12.556-.007.69-.282v-.002c.02-.039.046-.09.062-.127.07-.162.094-.301.144-.458.132-.332.205-.68.387-.897.05-.06.13-.082.215-.105l.113-.205a6.453 6.453 0 0 0 4.609.012l.106.192c.086.028.18.042.256.155.136.232.229.507.342.84.05.156.074.295.145.457.016.037.043.09.062.129.133.276.442.402.69.282.247-.118.341-.435.213-.71-.02-.039-.045-.096-.065-.134-.083-.156-.177-.261-.268-.398-.19-.296-.346-.541-.443-.793-.04-.13.007-.21.038-.294-.018-.022-.059-.144-.083-.202a6.499 6.499 0 0 0 2.88-3.622c.064.01.176.03.213.038.075-.05.144-.114.28-.104.266.039.54.138.87.256.154.06.277.128.448.173.036.01.088.019.13.028l.009.003.007.001c.297.064.584-.098.645-.365.06-.266-.128-.537-.423-.608zM16.4 9.701l-1.95 1.746v.005a.44.44 0 0 0 .173.757l.003.01 2.526.728a5.199 5.199 0 0 0-.108-1.674A5.208 5.208 0 0 0 16.4 9.7zm-4.013 5.325a.437.437 0 0 0-.404-.232.44.44 0 0 0-.372.233h-.002l-1.268 2.292a5.164 5.164 0 0 0 3.326.003l-1.27-2.296h-.01zm1.888-1.293a.44.44 0 0 0-.27.036.44.44 0 0 0-.214.572l-.003.004 1.01 2.438a5.15 5.15 0 0 0 2.081-2.615l-2.6-.44-.004.005z"/></svg>}
    href="/latest/operator/overview"
  >
  Automatically inject instrumentation into existing workloads without modifying pod specs, container images, or application code.
  </Card>