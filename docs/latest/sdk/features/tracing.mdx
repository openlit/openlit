---
title: "Tracing"
description: "Automatically track your AI apps with OpenTelemetry to gain insights into performance and behavior"
---

OpenLIT provides OpenTelemetry Auto instrumentation for various LLM providers, frameworks, and VectorDBs, providing you with insights into the behavior and performance of your LLM applications.

You can enable tracing with **zero code changes** using the CLI or by adding 2 lines of code with the SDK. This documentation covers tracing settings, understanding semantic conventions, and interpreting span attributes to enhance the monitoring and observability of your LLM applications.

## Getting Started

<Tabs>
  <Tab title="Zero Code Changes (CLI)">
    Perfect for existing applications where you cannot or don't want to modify code:
    
    ```bash
    # Install and run with zero code changes
    pip install openlit
    openlit-instrument python your_existing_app.py
    
    # Configure tracing options via CLI
    openlit-instrument \
      --service-name my-ai-service \
      --environment production \
      --disable-batch \
      --no-capture-message-content \
      python your_app.py
    ```
  </Tab>
  
  <Tab title="SDK Integration">
    For applications where you can modify code:
    
    ```python
    import openlit
    
    # Initialize with custom tracing settings
    openlit.init(
        service_name="my-ai-service",
        environment="production",
        disable_batch=True,
        capture_message_content=False
    )
    
    # Your existing AI code works without changes
    ```
  </Tab>
</Tabs>

<CardGroup cols={2}>
  <Card title="Quickstart" href="/latest/quickstart" icon="bolt">
    Get Started with monitoring your AI Applications in 2 simple steps
  </Card>
  <Card title="Connections" href="/latest/sdk/destinations/intro" icon="link">
    Connect to your existing Observablity Stack
  </Card>
</CardGroup>

## Using an existing OTel Tracer

You have the flexibility to integrate your existing OpenTelemetry (OTel) tracer configuration with OpenLIT.
If you already have an OTel tracer instantiated in your application, you can pass it directly to `openlit.init(tracer=tracer)`.
This integration ensures that OpenLIT utilizes your custom tracer settings, allowing for a unified tracing setup across your application.

Example:

```python
# Instantiate an OpenTelemetry Tracer
tracer = ...

# Pass the tracer to OpenLIT
openlit.init(tracer=tracer)
```

## Add custom resource attributes

The [`OTEL_RESOURCE_ATTRIBUTES`](https://opentelemetry.io/docs/languages/sdk-configuration/general/#otel_resource_attributes) environment variable allows you to provide additional OpenTelemetry resource attributes when starting your application with OpenLIT. OpenLIT already includes some default resource attributes:

- `telemetry.sdk.name: openlit`
- `service.name: YOUR_SERVICE_NAME`
- `deployment.environment: YOUR_ENVIRONMENT_NAME`

You can enhance these default resource attributes by adding your own using the `OTEL_RESOURCE_ATTRIBUTES` variable. Your custom attributes will be added on top of the existing OpenLIT attributes, providing additional context to your telemetry data. Simply format your attributes as `key1=value1,key2=value2`.

For example:

```shell
export OTEL_RESOURCE_ATTRIBUTES="service.instance.id=YOUR_SERVICE_ID,k8s.pod.name=K8S_POD_NAME,k8s.namespace.name=K8S_NAMESPACE,k8s.node.name=K8S_NODE_NAME"
```

## Disable Tracing of Content

By default, OpenLIT adds the prompts and completions to Trace span attributes.

However, you may want to disable this logging for privacy reasons, as they may contain highly sensitive data from your users. You may also simply want to reduce the size of your traces.

<Tabs>
  <Tab title="CLI">
    ```bash
    # Disable content capture with CLI
    openlit-instrument --no-capture-message-content python your_app.py
    
    # Or via environment variable
    export OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=false
    openlit-instrument python your_app.py
    ```
  </Tab>
  
  <Tab title="SDK">
    ```python
    openlit.init(capture_message_content=False)
    ```
  </Tab>
</Tabs>

## Disable Batch

By default, the SDK batches spans using the OpenTelemetry batch span processor. When working locally, sometimes you may wish to disable this behavior. You can do that with this flag.

<Tabs>
  <Tab title="CLI">
    ```bash
    # Disable batching with CLI
    openlit-instrument --disable-batch python your_app.py
    
    # Or via environment variable
    export OPENLIT_DISABLE_BATCH=true
    openlit-instrument python your_app.py
    ```
  </Tab>
  
  <Tab title="SDK">
    ```python
    openlit.init(disable_batch=True)
    ```
  </Tab>
</Tabs>

## Disable Instrumentations

By default, OpenLIT automatically detects which models and frameworks you are using and instruments them for you. You can override this and disable instrumentation for specific frameworks and models.

<Tabs>
  <Tab title="CLI">
    ```bash
    # Disable specific instrumentations with CLI
    openlit-instrument --disabled-instrumentors anthropic,langchain python your_app.py
    
    # Or via environment variable
    export OPENLIT_DISABLED_INSTRUMENTORS=anthropic,langchain
    openlit-instrument python your_app.py
    ```
  </Tab>
  
  <Tab title="SDK">
    ```python
    openlit.init(disabled_instrumentors=["anthropic", "langchain"])
    ```
  </Tab>
</Tabs>

## Manual Tracing

Using `openlit.trace`, you get access to manually create traces, allowing you to record every process within a single function.

```python python
@openlit.trace
def generate_one_liner():
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": "Return a one liner from any movie for me to guess",
            }
        ],
    )
```

The `trace` function automatically groups any LLM function invoked within `generate_one_liner`, providing you with organized groupings right out of the box.

You can do more with traces by running the `start_trace` context generator:

```python python
with openlit.start_trace(name="<GIVE_TRACE_A_NAME>") as trace:
    # your code
```

Use `trace.set_result('')` to set the final result of the trace and `trace.set_metadata({})` to add custom metadata.

**Full Example**

```python python
@openlit.trace
def generate_one_liner():
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": "Return a one liner from any movie for me to guess",
            }
        ],
    )

def guess_one_liner(one_liner: str):
    with openlit.start_trace("Guess One-liner") as trace:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "user",
                    "content": f"Guess movie from this line: {one_liner}",
                }
            ],
        )
        trace.set_result(completion.choices[0].message.content)
```

## Semantic Convention

This section outlines the OpenTelemetry traces collected by **OpenLIT** from applications utilizing Large Language Models (LLMs) and Vector Databases. The span attributes adhere to the [GenAI Semantic Conventions](https://github.com/open-telemetry/semantic-conventions/tree/main/docs/gen-ai) established by the OpenTelemetry community, ensuring standardized data collection that enhances monitoring and debugging capabilities.

<CardGroup cols={2}>
  <Card
    title="Integrations"
    href="/latest/sdk/integrations/introduction"
    icon="circle-nodes"
  >
    Integrate your AI Stack with OpenLIT
  </Card>
  <Card title="Connections" href="/latest/sdk/destinations/intro" icon="link">
    Connect to your existing Observablity Stack
  </Card>
</CardGroup>