---
title: 'Configuration'
description: 'Configure the OpenLIT SDK for OpenTelemetry-native LLM observability, cost tracking, and performance monitoring'
icon: 'sliders'
---

Configure OpenLIT SDK for AI monitoring and model performance tracking using flexible instrumentation methods. Choose from Manual instrumentation or Zero-code instrumentation for complete LLM observability:

<Tabs>
<Tab title="Python">

### Manual instrumentation (SDK)

```python
import openlit

openlit.init(
    service_name="my-ai-app",
    environment="production",
    otlp_endpoint="https://otel-endpoint.com"
)
```

### Zero-code instrumentation (CLI)

```bash
export OTEL_SERVICE_NAME=my-ai-app
export OTEL_DEPLOYMENT_ENVIRONMENT=production
openlit-instrument python your_app.py
```

### Configuration parameters

Customize OpenLIT SDK behavior for your specific instrumentation needs:

| Parameter | CLI Argument | Environment Variable | Description | Default | Required |
|-----------|--------------|---------------------|-------------|---------|----------|
| `environment` | `--environment` | `OTEL_DEPLOYMENT_ENVIRONMENT` | Deployment environment | `"default"` | No |
| `service_name` | `--service_name` | `OTEL_SERVICE_NAME` | Service name for tracing | `"default"` | No |
| `otlp_endpoint` | `--otlp_endpoint` | `OTEL_EXPORTER_OTLP_ENDPOINT` | OpenTelemetry endpoint for LLM monitoring data export | `None` | No |
| `otlp_headers` | `--otlp_headers` | `OTEL_EXPORTER_OTLP_HEADERS` | Authentication headers for enterprise monitoring backends | `None` | No |
| `disable_batch` | `--disable_batch` | `OPENLIT_DISABLE_BATCH` | Disable batch span processing | `False` | No |
| `capture_message_content` | `--capture_message_content` | `OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT` | Enable LLM prompt and response content tracing for debugging | `True` | No |
| `disabled_instrumentors` | `--disabled_instrumentors` | `OPENLIT_DISABLED_INSTRUMENTORS` | Disable specific AI service instrumentations (comma-separated) | `None` | No |
| `disable_metrics` | `--disable_metrics` | `OPENLIT_DISABLE_METRICS` | Disable cost tracking and performance metrics collection | `False` | No |
| `pricing_json` | `--pricing_json` | `OPENLIT_PRICING_JSON` | Custom pricing configuration for accurate LLM cost tracking | `None` | No |
| `detailed_tracing` | `--detailed_tracing` | `OPENLIT_DETAILED_TRACING` | Enable detailed AI framework and component-level tracing | `True` | No |
| `collect_system_metrics` | `--collect_system_metrics` | `OPENLIT_COLLECT_SYSTEM_METRICS` | Comprehensive system monitoring (CPU, memory, disk, network, GPU) for AI workloads | `False` | No |
| `tracer` | N/A | N/A | An instance of OpenTelemetry Tracer for tracing operations | `None` | No |
| `event_logger` | N/A | N/A | EventLoggerProvider instance | `None` | No |
| `meter` | N/A | N/A | OpenTelemetry Metrics instance | `None` | No |

### Deprecated parameters

| Parameter | CLI Argument | Environment Variable | Description | Default | Required |
|-----------|--------------|---------------------|-------------|---------|----------|
| `application_name` | `--application_name` | `OTEL_SERVICE_NAME` | Application name for tracing (**deprecated**, use `service_name`) | `"default"` | No |
| `collect_gpu_stats` | `--collect_gpu_stats` | `OPENLIT_COLLECT_GPU_STATS` | Enable GPU statistics collection (**deprecated**, use `collect_system_metrics`) | `False` | No |

<Info>Environment variables take precedence over CLI arguments, which take precedence over SDK parameters.</Info>

### Resource attributes

Additional resource attributes can be controlled using standard OpenTelemetry environment variables for enhanced metadata and observability context:

| Environment Variable | Description | Example |
|---------------------|-------------|---------|
| `OTEL_RESOURCE_ATTRIBUTES` | Key-value pairs for resource attributes | `service.version=1.0.0,deployment.environment=production` |
| `OTEL_SERVICE_VERSION` | Version of the service | `1.2.3` |
| `OTEL_RESOURCE_ATTRIBUTES_POD_NAME` | Kubernetes pod name (if applicable) | `my-ai-app-pod-xyz` |
| `OTEL_RESOURCE_ATTRIBUTES_NODE_NAME` | Kubernetes node name (if applicable) | `node-123` |

**Example:**
```bash
# Set resource attributes for better trace organization
export OTEL_RESOURCE_ATTRIBUTES="service.version=2.1.0,team=ai-platform,cost.center=engineering"
export OTEL_SERVICE_VERSION=2.1.0

# Run with enhanced metadata
openlit-instrument python your_ai_app.py
```

These attributes enhance trace metadata for better filtering, grouping, and analysis in your observability platform.

## Prompt Hub - `openlit.get_prompt()`

Advanced prompt management and version control for production LLM applications. Configure OpenLIT Prompt Hub for centralized prompt governance and tracking:

| Parameter         | Description                                                                                                                        |
|-------------------|------------------------------------------------------------------------------------------------------------------------------------|
| `url`             | Sets the OpenLIT URL. Defaults to the `OPENLIT_URL` environment variable.                                                          |
| `api_key`         | Sets the OpenLIT API Key. Can also be provided via the `OPENLIT_API_KEY` environment variable.                                     |
| `name`            | Unique prompt identifier for retrieval. Use with `prompt_id` for specific prompt versioning                                        |
| `prompt_id`       | Numeric ID for direct prompt access. Enables precise prompt version control. Optional                                              |
| `version`         | Specific prompt version retrieval for consistent AI behavior across deployments. Optional                                          |
| `shouldCompile`   | Enable dynamic prompt compilation with variables for personalized LLM interactions. Optional                                       |
| `variables`       | Dynamic variables for prompt template compilation and customization. Optional                                                      |
| `meta_properties` | Tracking metadata for prompt usage analytics and audit trails in production. Optional                                              |

## Vault - `openlit.get_secrets()`

Enterprise-grade secret management for AI applications. Configure OpenLIT Vault for secure API key and credential handling in production LLM deployments:

| Parameter         | Description                                                                                                                        |
|-------------------|------------------------------------------------------------------------------------------------------------------------------------|
| `url`             | Sets the Openlit URL. Defaults to the `OPENLIT_URL` environment variable.                                                          |
| `api_key`         | Sets the OpenLIT API Key. Can also be provided via the `OPENLIT_API_KEY` environment variable.                                     |
| `key`            | Specific secret key retrieval for individual credential access. Optional                                                           |
| `should_set_env` | Automatically set retrieved secrets as environment variables for seamless application integration. Optional                        |
| `tags`           | Tag-based secret filtering for organized credential management across different AI services. Optional                               |

</Tab>
<Tab title="Typescript">

## Configuration parameters

Customize OpenLIT SDK behavior for your specific instrumentation needs:

| Argument                 | Description                                                                          | Default Value                                                      | Required |
| ------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------ | -------- |
| `environment`            | The deployment environment of the application.                                       | `"default"`                                                        | No      |
| `applicationName`       | Identifies the name of your application.                                             | `"default"`                                                        | No      |
| `tracer`                 | An instance of OpenTelemetry Tracer for tracing operations.                          | `undefined`                                                             | No       |
| `otlpEndpoint`          | OpenTelemetry endpoint for LLM monitoring data transmission to enterprise backends.  | `undefined`                                                             | No       |
| `otlpHeaders`           | Authentication headers for secure integration with monitoring platforms like Datadog, Grafana. | `undefined`                                                             | No       |
| `disableBatch`          | A flag to disable batch span processing, favoring immediate dispatch.                | `true`                                                            | No       |
| `traceContent`          | Enable LLM prompt and response content capture for comprehensive debugging.          | `true`                                                             | No       |
| `disabledInstrumentations` | Disable specific AI service instrumentations for customized monitoring scope.       | `undefined`                                                             | No       |
| `instrumentations`        | Object of instrumentation modules for manual patching                                          | `undefined`                                                            | No       |
| `pricing_json`           | Custom pricing configuration URL for accurate LLM cost tracking and optimization.    | `https://github.com/openlit/openlit/blob/main/assets/pricing.json` | No       |

### Resource attributes

Additional resource attributes can be controlled using standard OpenTelemetry environment variables for enhanced metadata and observability context:

| Environment Variable | Description | Example |
|---------------------|-------------|---------|
| `OTEL_RESOURCE_ATTRIBUTES` | Key-value pairs for resource attributes | `service.version=1.0.0,deployment.environment=production` |
| `OTEL_SERVICE_VERSION` | Version of the service | `1.2.3` |
| `OTEL_RESOURCE_ATTRIBUTES_POD_NAME` | Kubernetes pod name (if applicable) | `my-ai-app-pod-xyz` |
| `OTEL_RESOURCE_ATTRIBUTES_NODE_NAME` | Kubernetes node name (if applicable) | `node-123` |

These attributes enhance trace metadata for better filtering, grouping, and analysis in your observability platform.

## Prompt Hub - `Openlit.getPrompt()`

Advanced prompt management for Node.js AI applications. Configure centralized prompt governance and version control for production LLM deployments:

| Parameter        | Description                                                                                                                        |
|------------------|------------------------------------------------------------------------------------------------------------------------------------|
| `url`            | Sets the OpenLIT URL. Defaults to the `OPENLIT_URL` environment variable or `http://127.0.0.1:3000` if not set.                    |
| `apiKey`         | Sets the OpenLIT API Key. Can also be provided via the `OPENLIT_API_KEY` environment variable.                                     |
| `name`           | Unique prompt identifier for consistent retrieval across deployments.                                                              |
| `promptId`       | Numeric prompt ID for direct access and version control. Optional                                                                  |
| `version`        | Specific prompt version for consistent AI behavior in production. Optional                                                          | 
| `shouldCompile`  | Enable dynamic prompt compilation with variables for personalized interactions. Optional                                           |
| `variables`      | Dynamic template variables for prompt customization and personalization. Optional                                                  | 
| `metaProperties` | Usage tracking metadata for prompt analytics and audit trails. Optional                                                            | 


## Vault - `Openlit.getSecrets()`

Enterprise-grade secret management for Node.js AI applications. Configure secure credential handling for production LLM deployments:

| Parameter        | Description                                                                                                  |
|------------------|------------------------------------------------------------------------------------------------------------------------------------|
| `url`            | Sets the Openlit URL. Defaults to the `OPENLIT_URL` environment variable or `http://127.0.0.1:3000` if not set.                    |
| `apiKey`         | Sets the OpenLIT API Key. Can also be provided via the `OPENLIT_API_KEY` environment variable.                                     |
| `key`            | Specific secret key for individual credential retrieval. Optional                                                  |
| `tags`           | Tag-based secret filtering for organized credential management across AI services. Optional                         |
| `shouldSetEnv`   | Automatically set secrets as environment variables for seamless application integration. Optional                   |

</Tab>
</Tabs>

---

<CardGroup cols={3}>
  <Card title="Quickstart: LLM Guardrails" href="/latest/sdk/quickstart-programmatic-evals" icon='bolt'>
    Protect and secure your LLM responses in 2 simple steps
  </Card>
  <Card title="Integrations" href="/latest/sdk/integrations/overview" icon='circle-nodes'>
    60+ AI integrations with automatic instrumentation and performance tracking
  </Card>
  <Card title="Destinations" href="/latest/sdk/destinations/overview" icon='link'>
    Send elemetry to Datadog, Grafana, New Relic, and other observability stacks
  </Card>
</CardGroup>
<Card
    title="Running in Kubernetes? Try the OpenLIT Operator"
    icon={<svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
    <title>Kubernetes</title>
    <path fill="#FFA500" d="M10.204 14.35l.007.01-.999 2.413a5.171 5.171 0 0 1-2.075-2.597l2.578-.437.004.005a.44.44 0 0 1 .484.606zm-.833-2.129a.44.44 0 0 0 .173-.756l.002-.011L7.585 9.7a5.143 5.143 0 0 0-.73 3.255l2.514-.725.002-.009zm1.145-1.98a.44.44 0 0 0 .699-.337l.01-.005.15-2.62a5.144 5.144 0 0 0-3.01 1.442l2.147 1.523.004-.002zm.76 2.75l.723.349.722-.347.18-.78-.5-.623h-.804l-.5.623.179.779zm1.5-3.095a.44.44 0 0 0 .7.336l.008.003 2.134-1.513a5.188 5.188 0 0 0-2.992-1.442l.148 2.615.002.001zm10.876 5.97l-5.773 7.181a1.6 1.6 0 0 1-1.248.594l-9.261.003a1.6 1.6 0 0 1-1.247-.596l-5.776-7.18a1.583 1.583 0 0 1-.307-1.34L2.1 5.573c.108-.47.425-.864.863-1.073L11.305.513a1.606 1.606 0 0 1 1.385 0l8.345 3.985c.438.209.755.604.863 1.073l2.062 8.955c.108.47-.005.963-.308 1.34zm-3.289-2.057c-.042-.01-.103-.026-.145-.034-.174-.033-.315-.025-.479-.038-.35-.037-.638-.067-.895-.148-.105-.04-.18-.165-.216-.216l-.201-.059a6.45 6.45 0 0 0-.105-2.332 6.465 6.465 0 0 0-.936-2.163c.052-.047.15-.133.177-.159.008-.09.001-.183.094-.282.197-.185.444-.338.743-.522.142-.084.273-.137.415-.242.032-.024.076-.062.11-.089.24-.191.295-.52.123-.736-.172-.216-.506-.236-.745-.045-.034.027-.08.062-.111.088-.134.116-.217.23-.33.35-.246.25-.45.458-.673.609-.097.056-.239.037-.303.033l-.19.135a6.545 6.545 0 0 0-4.146-2.003l-.012-.223c-.065-.062-.143-.115-.163-.25-.022-.268.015-.557.057-.905.023-.163.061-.298.068-.475.001-.04-.001-.099-.001-.142 0-.306-.224-.555-.5-.555-.275 0-.499.249-.499.555l.001.014c0 .041-.002.092 0 .128.006.177.044.312.067.475.042.348.078.637.056.906a.545.545 0 0 1-.162.258l-.012.211a6.424 6.424 0 0 0-4.166 2.003 8.373 8.373 0 0 1-.18-.128c-.09.012-.18.04-.297-.029-.223-.15-.427-.358-.673-.608-.113-.12-.195-.234-.329-.349-.03-.026-.077-.062-.111-.088a.594.594 0 0 0-.348-.132.481.481 0 0 0-.398.176c-.172.216-.117.546.123.737l.007.005.104.083c.142.105.272.159.414.242.299.185.546.338.743.522.076.082.09.226.1.288l.16.143a6.462 6.462 0 0 0-1.02 4.506l-.208.06c-.055.072-.133.184-.215.217-.257.081-.546.11-.895.147-.164.014-.305.006-.48.039-.037.007-.09.02-.133.03l-.004.002-.007.002c-.295.071-.484.342-.423.608.061.267.349.429.645.365l.007-.001.01-.003.129-.029c.17-.046.294-.113.448-.172.33-.118.604-.217.87-.256.112-.009.23.069.288.101l.217-.037a6.5 6.5 0 0 0 2.88 3.596l-.09.218c.033.084.069.199.044.282-.097.252-.263.517-.452.813-.091.136-.185.242-.268.399-.02.037-.045.095-.064.134-.128.275-.034.591.213.71.248.12.556-.007.69-.282v-.002c.02-.039.046-.09.062-.127.07-.162.094-.301.144-.458.132-.332.205-.68.387-.897.05-.06.13-.082.215-.105l.113-.205a6.453 6.453 0 0 0 4.609.012l.106.192c.086.028.18.042.256.155.136.232.229.507.342.84.05.156.074.295.145.457.016.037.043.09.062.129.133.276.442.402.69.282.247-.118.341-.435.213-.71-.02-.039-.045-.096-.065-.134-.083-.156-.177-.261-.268-.398-.19-.296-.346-.541-.443-.793-.04-.13.007-.21.038-.294-.018-.022-.059-.144-.083-.202a6.499 6.499 0 0 0 2.88-3.622c.064.01.176.03.213.038.075-.05.144-.114.28-.104.266.039.54.138.87.256.154.06.277.128.448.173.036.01.088.019.13.028l.009.003.007.001c.297.064.584-.098.645-.365.06-.266-.128-.537-.423-.608zM16.4 9.701l-1.95 1.746v.005a.44.44 0 0 0 .173.757l.003.01 2.526.728a5.199 5.199 0 0 0-.108-1.674A5.208 5.208 0 0 0 16.4 9.7zm-4.013 5.325a.437.437 0 0 0-.404-.232.44.44 0 0 0-.372.233h-.002l-1.268 2.292a5.164 5.164 0 0 0 3.326.003l-1.27-2.296h-.01zm1.888-1.293a.44.44 0 0 0-.27.036.44.44 0 0 0-.214.572l-.003.004 1.01 2.438a5.15 5.15 0 0 0 2.081-2.615l-2.6-.44-.004.005z"/></svg>}
    href="/latest/operator/overview"
  >
  Automatically inject instrumentation into existing workloads without modifying pod specs, container images, or application code.
  </Card>