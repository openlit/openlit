# Framework Instrumentation Guide for OpenLIT

This guide provides a comprehensive, step-by-step process for adding new framework instrumentations or updating existing ones in OpenLIT. It's based on lessons learned from successful instrumentations including OpenAI Agents, Haystack, and others, ensuring consistency, performance, and competitive advantages across all framework integrations.

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Phase 1: Competitive Analysis](#phase-1-competitive-analysis)
3. [Phase 2: Framework Analysis](#phase-2-framework-analysis)
4. [Phase 3: Implementation Strategy](#phase-3-implementation-strategy)
5. [Phase 4: Code Implementation](#phase-4-code-implementation)
6. [Phase 5: Testing & Validation](#phase-5-testing--validation)
7. [Phase 6: Optimization](#phase-6-optimization)
8. [Phase 7: Post-Change Cleanup](#phase-7-post-change-cleanup)
9. [Code Standards & Patterns](#code-standards--patterns)
10. [Quality Checklist](#quality-checklist)

## Prerequisites

Before starting, ensure you have:
- OpenLIT development environment set up
- Target framework installed and working examples
- Access to competitor repositories for analysis
- Understanding of OpenTelemetry concepts

**CRITICAL**: Always use local OpenLIT source code for testing:
```bash
# DON'T install OpenLIT - use local source
PYTHONPATH=sdk/python/src python your_test_script.py
```

## Phase 1: Competitive Analysis

### Step 1.1: Clone and Research Competitor Implementations

**Primary Competitors:**
- [OpenInference](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation)
- [OpenLLMetry](https://github.com/traceloop/openllmetry/tree/main/packages)
- [AgentOps](https://github.com/AgentOps-AI/agentops)
- [LangFuse](https://github.com/langfuse/langfuse) (Not OpenTelemetry native)
- [LangSmith](https://github.com/langchain-ai/langsmith-sdk)

**Research Process:**
```bash
# Clone competitor repositories for deep analysis
mkdir competitive_analysis && cd competitive_analysis
git clone https://github.com/Arize-ai/openinference.git
git clone https://github.com/traceloop/openllmetry.git
git clone https://github.com/AgentOps-AI/agentops.git
git clone https://github.com/langfuse/langfuse.git
git clone https://github.com/langchain-ai/langsmith-sdk.git

# Navigate to framework-specific instrumentations
cd openinference/python/instrumentation/openinference-instrumentation-{framework}
cd openllmetry/packages/openllmetry-instrumentation-{framework}
```

**Deep Analysis Checklist:**
- [ ] **Integration Pattern**: Function wrapping vs native integration (like TracingProcessor)
- [ ] **Span Structure**: How many spans do they create and why?
- [ ] **Span Hierarchy**: Do they maintain proper parent-child relationships?
- [ ] **Span Naming**: What naming convention do they use?
- [ ] **Attributes**: What attributes do they capture? Check against semantic conventions
- [ ] **Content Capture**: Do they capture input/output content with MIME types?
- [ ] **Business Intelligence**: Do they track cost, tokens, performance metrics?
- [ ] **Error Handling**: How do they handle framework version differences?
- [ ] **Performance**: How much overhead do they add?
- [ ] **Coverage**: Which framework operations do they instrument?

### Step 1.2: Document Competitive Gaps and OpenLIT Advantages

Create detailed comparison:

| Feature | OpenInference | OpenLLMetry | OpenLIT Target |
|---------|---------------|-------------|----------------|
| Integration Method | Function wrapping | Function wrapping | Native (if available) |
| Span Count | X spans | Y spans | Z+ spans (comprehensive) |
| Business Intelligence | None/Basic | None/Basic | **Complete** (cost, tokens, metrics) |
| Content Capture | Basic | Basic | **Enhanced** (MIME types, structured) |
| Cross-System Tracing | Limited | Limited | **Full** (links to LLM providers) |
| Error Resilience | Basic | Basic | **Robust** (graceful degradation) |
| Semantic Conventions | Standard | Standard | **Extended** (custom attributes) |

**Key Insight**: OpenLIT's competitive advantage is **business intelligence** and **enhanced observability**.

### Step 1.2: Instrumentation Structure Patterns

**FLEXIBLE file structures based on framework complexity:**

```python
# PATTERN 1: Single File (Simple frameworks)
# Example: Simple utilities, basic wrappers
framework_name/
‚îú‚îÄ‚îÄ __init__.py      # Instrumentation setup + all logic
‚îî‚îÄ‚îÄ (optional utils if needed)

# PATTERN 2: Processor Pattern (OpenAI Agents style)
# Example: Pipeline/processing-based frameworks
framework_name/
‚îú‚îÄ‚îÄ __init__.py      # Instrumentation setup
‚îî‚îÄ‚îÄ processor.py     # All processing logic, context handling

# PATTERN 3: Callback Pattern (LangChain style)
# Example: Frameworks with built-in callback systems
framework_name/
‚îú‚îÄ‚îÄ __init__.py      # Instrumentation setup
‚îî‚îÄ‚îÄ callback_handler.py # Framework's callback interface

# PATTERN 4: Standard Wrapper Pattern (CrewAI, Pydantic AI)
# Example: Most agent frameworks
framework_name/
‚îú‚îÄ‚îÄ __init__.py      # Instrumentation setup
‚îú‚îÄ‚îÄ framework_name.py # Sync wrappers
‚îú‚îÄ‚îÄ async_framework_name.py # Async wrappers
‚îî‚îÄ‚îÄ utils.py         # Shared utilities with context caching

```

### Step 1.3: Performance Optimization Patterns

**CRITICAL**: Study existing OpenLIT implementations for optimization patterns:

```python
# Study these proven patterns in existing instrumentations:
# - CrewAI: excellent agent patterns, lifecycle management
# - OpenAI Agents: great tool handling (processor.py pattern)
# - LangChain/LlamaIndex: mature framework patterns, utils.py caching
# - Pydantic AI: context caching with PydanticAIInstrumentationContext

# Example: Context caching pattern from Pydantic AI
class FrameworkInstrumentationContext:
    """Context object to cache expensive extractions."""
    
    def __init__(self, instance, args, kwargs, version, environment, application_name):
        self.instance = instance
        self.args = args
        self.kwargs = kwargs
        
        # Cache expensive operations with lazy loading
        self._agent_name = None
        self._model_name = None
        self._tools = None
        self._messages = None
    
    @property
    def agent_name(self) -> str:
        """Lazy-loaded with caching."""
        if self._agent_name is None:
            self._agent_name = self._extract_agent_name()
        return self._agent_name
```

## Phase 2: Framework Analysis

### Step 2.1: Explore Framework's Built-in Capabilities

**CRITICAL**: Before implementing, check if the framework has built-in tracing/monitoring:

```python
# Example: Discover OpenAI Agents has native tracing
import {framework}

# Look for tracing, monitoring, observability features
# Check documentation for:
# - Built-in tracing systems
# - Processor patterns  
# - Event hooks
# - Monitoring callbacks

# Example: OpenAI Agents has TracingProcessor
from agents import TracingProcessor, set_trace_processors
```

**Framework Integration Decision Tree:**
1. **Native Integration Available** (e.g., OpenAI Agents TracingProcessor)
   - ‚úÖ Use native integration for perfect hierarchy
   - ‚úÖ Extends framework's built-in system
   - ‚úÖ Better performance and reliability

2. **Function Wrapping Required** (most frameworks)
   - ‚úÖ Standard approach for frameworks without native tracing
   - ‚ö†Ô∏è Requires careful hierarchy management

### Step 2.2: Clone and Study Target SDK (CRITICAL)

**ALWAYS clone the actual SDK you're instrumenting:**

### Step 2.3: Semantic Conventions Are CRITICAL

**NEVER hardcode strings - ALWAYS use SemanticConvention:**

```python
# ‚ùå WRONG - will be rejected in code review:
span.set_attribute("gen_ai.operation.name", "chat")
span.set_attribute("gen_ai.system", "pydantic_ai")

# ‚úÖ CORRECT - must use semantic conventions:
from openlit.semcov import SemanticConvention

span.set_attribute(SemanticConvention.GEN_AI_OPERATION, SemanticConvention.GEN_AI_OPERATION_TYPE_CHAT)
span.set_attribute(SemanticConvention.GEN_AI_SYSTEM, SemanticConvention.GEN_AI_SYSTEM_PYDANTIC_AI)

# Check existing constants first:
# - GEN_AI_OPERATION_TYPE_* 
# - GEN_AI_AGENT_LIFECYCLE_PHASE_*
# - GEN_AI_SYSTEM_* (add new ones if needed)

# If needed, add to semcov.py:
class SemanticConvention:
    # Add framework-specific constants
    GEN_AI_SYSTEM_PYDANTIC_AI = "pydantic_ai"
    GEN_AI_AGENT_LIFECYCLE_PHASE_GRAPH_EXECUTION = "graph_execution"
```

### Step 2.4: Map Framework Operations

**Workflow vs Component Operations:**
```python
# Example for any framework
WORKFLOW_OPERATIONS = [
    # High-level operations users care about in production
    "agent.run_sync",     # OpenAI Agents
    "pipeline.run",       # Haystack  
    "workflow.execute"    # Generic
]

COMPONENT_OPERATIONS = [
    # Detailed operations for debugging (detailed_tracing=True)
    "retriever.retrieve",
    "generator.generate", 
    "embedder.embed",
    "tool.call"
]
```

### Step 2.3: Test Framework Methods and Patterns

**Method Discovery Script:**
```python
# test_framework_exploration.py
import {framework}

# Discover available methods and attributes
obj = SomeFrameworkClass()
print("Available methods:", [m for m in dir(obj) if not m.startswith('_')])

# Test built-in monitoring
if hasattr(obj, 'trace') or hasattr(obj, 'monitor'):
    print("Built-in tracing available!")
    
# Check for callback patterns
if hasattr(obj, 'add_callback') or hasattr(obj, 'on_event'):
    print("Event system available!")
```

## Phase 3: Implementation Strategy

### Step 3.1: Choose Integration Pattern

**Pattern 1: Native Integration (Preferred when available)**
```python
# Example: OpenAI Agents TracingProcessor
class OpenLITTracingProcessor(TracingProcessor):
    def on_trace_start(self, trace): pass
    def on_span_start(self, span): pass  
    def on_span_end(self, span): pass
```

**Pattern 2: Function Wrapping (Standard)**
```python
# Traditional OpenLIT pattern
wrap_function_wrapper("module", "Class.method", wrapper)
```

### Step 3.2: Plan Span Naming Convention

**CRITICAL**: Follow consistent naming pattern:
```python
# Standard format: "{operation_type} {operation_name}"
"agent Triage agent"      # operation_type=agent, operation_name=Triage agent
"chat gpt-4o"            # operation_type=chat, operation_name=gpt-4o  
"retrieve documents"      # operation_type=retrieve, operation_name=documents
"generate response"       # operation_type=generate, operation_name=response
```

### Step 3.3: Plan Semantic Conventions Usage

**Extend semcov.py when needed:**
```python
# It's OK to add new semantic conventions for better attributes
class SemanticConvention:
    # Add framework-specific conventions
    GEN_AI_AGENT_DESCRIPTION = "gen_ai.agent.description"
    GEN_AI_AGENT_VERSION = "gen_ai.agent.version"
    GEN_AI_WORKFLOW_TYPE = "gen_ai.workflow.type"
```

## Phase 4: Code Implementation

### Step 4.1: Create Test Infrastructure First

**Create span hierarchy test before implementing:**
```python
# test_span_hierarchy.py - Always create this first
import sys
sys.path.insert(0, 'sdk/python/src')  # Use local source

import openlit
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

class CollectingSpanExporter:
    def __init__(self):
        self.spans = []
    
    def export(self, spans):
        for span in spans:
            self.spans.append({
                'name': span.name,
                'parent_id': format(span.parent.span_id, '016x') if span.parent else None,
                'attributes': dict(span.attributes) if span.attributes else {}
            })
        return 0

def print_span_hierarchy(spans):
    """Print spans in hierarchical tree structure"""
    # Build parent-child relationships
    root_spans = [s for s in spans if s['parent_id'] is None]
    child_map = {}
    for span in spans:
        if span['parent_id']:
            child_map.setdefault(span['parent_id'], []).append(span)
    
    def print_span(span, level=0):
        indent = "  " * level
        prefix = "‚îú‚îÄ‚îÄ " if level > 0 else ""
        print(f"{indent}{prefix}{span['name']}")
        
        span_id = format(int(span['parent_id'] or 0), '016x')
        for child in child_map.get(span_id, []):
            print_span(child, level + 1)
    
    for root in root_spans:
        print_span(root)

def test_framework():
    collector = CollectingSpanExporter()
    tracer_provider = TracerProvider()
    tracer_provider.add_span_processor(SimpleSpanProcessor(collector))
    
    openlit.init(detailed_tracing=True)
    
    # Test framework operations here
    # ...
    
    print_span_hierarchy(collector.spans)
```

### Step 4.2: Implement Based on Integration Pattern

**For Native Integration:**
```python
# processor.py - When framework has built-in tracing
class OpenLITTracingProcessor(FrameworkProcessor):
    def on_span_start(self, span):
        # Create OpenTelemetry span with proper naming
        span_name = self._get_span_name(span)  # {operation_type} {operation_name}
        otel_span = self._tracer.start_span(span_name, kind=SpanKind.CLIENT)
        
        # Set semantic convention attributes
        self._set_common_attributes(otel_span, span)
        
    def _get_span_name(self, span):
        operation_type = self._get_operation_type(span.span_data)
        operation_name = self._extract_operation_name(span.span_data)
        return f"{operation_type} {operation_name}"
```

**For Function Wrapping:**
```python
# Follow existing 4-file structure
# __init__.py, sync_wrapper.py, async_wrapper.py, utils.py
```

### Step 4.3: Implement Comprehensive Attributes

**Use semantic conventions extensively:**
```python
def _set_span_attributes(self, span, data):
    # Standard framework attributes
    span.set_attribute(SemanticConvention.GEN_AI_SYSTEM, "framework_name")
    span.set_attribute(SemanticConvention.GEN_AI_OPERATION_NAME, operation_name)
    
    # Model information (critical for business intelligence)
    model = self._extract_model_info(data)
    if model:
        span.set_attribute(SemanticConvention.GEN_AI_REQUEST_MODEL, model)
    
    # Content capture with MIME types (OpenLIT enhancement)
    if self._capture_message_content:
        self._capture_input_output_with_mime_types(span, data)
    
    # Business intelligence attributes
    self._capture_token_usage(span, data)
    self._capture_cost_metrics(span, data)
    
    # Framework-specific attributes
    if hasattr(data, 'agent_name'):
        span.set_attribute(SemanticConvention.GEN_AI_AGENT_NAME, data.agent_name)
```

## Phase 5: Testing & Validation

### Step 5.1: Create Comprehensive Test Suite

**Test Categories:**
1. **Span Hierarchy Test** (most important)
2. **Competitive Comparison Test** (CRITICAL)
3. **Performance Benchmarking Test**
4. **Mock Framework Test** (when real framework unavailable)
5. **Error Resilience Test**

**Span Hierarchy Validation:**
```python
def test_span_hierarchy():
    """Validate proper parent-child relationships and span naming"""
    spans = run_instrumentation_test()
    
    # Validate hierarchy structure
    assert len(spans) > 0, "Must generate spans"
    
    # Check root span exists
    root_spans = [s for s in spans if s['parent_id'] is None]
    assert len(root_spans) == 1, "Must have exactly one root span"
    
    # Validate span naming convention
    for span in spans:
        assert ' ' in span['name'], f"Span name '{span['name']}' should follow '{{operation_type}} {{operation_name}}' format"
    
    # Check semantic conventions
    for span in spans:
        attrs = span['attributes']
        assert 'gen_ai.system' in attrs, "Must have gen_ai.system attribute"
```

### Step 5.2: Competitive Validation

**Critical Test - OpenLIT vs Competitors:**
```python
def test_competitive_superiority():
    """Validate OpenLIT generates more/better spans than competitors"""
    
    # Test OpenLIT
    openlit_spans = test_openlit_instrumentation()
    
    # Test competitors (with error handling)
    try:
        competitor1_spans = test_competitor_instrumentation("openinference")
        competitor2_spans = test_competitor_instrumentation("openllmetry")
        competitor3_spans = test_competitor_instrumentation("agentops")
    except Exception as e:
        print(f"Competitor failed: {e}")
        competitor1_spans = competitor2_spans = competitor3_spans = 0
    
    # Validate OpenLIT superiority
    assert openlit_spans > 0, "OpenLIT must generate spans"
    print(f"OpenLIT: {openlit_spans} spans (with business intelligence)")
    print(f"OpenInference: {competitor1_spans} spans") 
    print(f"OpenLLMetry: {competitor2_spans} spans")
    print(f"AgentOps: {competitor3_spans} spans")
    
    # OpenLIT should provide comprehensive coverage
    assert openlit_spans >= max(competitor1_spans, competitor2_spans, competitor3_spans)
```

### Step 5.3: Performance Benchmarking

**Critical Test - Performance vs Competitors:**
```python
def benchmark_instrumentation_performance():
    """Compare instrumentation overhead across competitors."""
    
    import time
    
    # Test scenarios
    test_cases = [
        "single_agent_simple_task",
        "multi_agent_conversation", 
        "tool_heavy_workflow",
        "high_frequency_requests"
    ]
    
    competitors = ["openlit", "openinference", "openllmetry", "agentops"]
    
    results = {}
    for competitor in competitors:
        results[competitor] = {}
        
        for test_case in test_cases:
            # Baseline without instrumentation
            start = time.time()
            run_test_case(test_case, instrumentation=None)
            baseline = time.time() - start
            
            # With competitor instrumentation
            start = time.time()
            run_test_case(test_case, instrumentation=competitor)
            instrumented = time.time() - start
            
            overhead = ((instrumented - baseline) / baseline) * 100
            results[competitor][test_case] = {
                "overhead_percent": overhead,
                "baseline_ms": baseline * 1000,
                "instrumented_ms": instrumented * 1000
            }
    
    # Validate OpenLIT performance
    openlit_avg_overhead = sum(results["openlit"][tc]["overhead_percent"] for tc in test_cases) / len(test_cases)
    assert openlit_avg_overhead < 10, f"OpenLIT overhead too high: {openlit_avg_overhead:.2f}%"
    
    return results
```

### Step 5.3: Mock Testing When Framework Unavailable

**Create realistic mock scenarios:**
```python
def test_with_mocks():
    """Test instrumentation when real framework isn't available"""
    tracer = trace.get_tracer(__name__)
    
    # Create realistic span hierarchy manually
    with tracer.start_as_current_span("agent Agent workflow") as root:
        root.set_attribute("gen_ai.agent.name", "Agent workflow")
        
        with tracer.start_as_current_span("agent Triage agent") as agent:
            agent.set_attribute("gen_ai.agent.name", "Triage agent")
            
            with tracer.start_as_current_span("chat gpt-4o") as chat:
                chat.set_attribute("gen_ai.request.model", "gpt-4o")
```

## Phase 6: Optimization

### Step 6.1: Context Caching for Performance

**Implement context caching pattern (learned from Pydantic AI optimization):**

```python
class FrameworkInstrumentationContext:
    """Context object to cache expensive extractions and reduce performance overhead."""
    
    def __init__(self, instance, args, kwargs, version, environment, application_name):
        self.instance = instance
        self.args = args
        self.kwargs = kwargs
        self.version = version
        self.environment = environment
        self.application_name = application_name
        
        # Cache expensive operations with lazy loading
        self._agent_name = None
        self._model_name = None
        self._server_info = None
        self._messages = None
        self._tools = None
        self._model_params = None
        
    @property
    def agent_name(self) -> str:
        """Get agent name with caching - avoids repeated extraction."""
        if self._agent_name is None:
            self._agent_name = getattr(self.instance, 'name', None) or "default_agent"
        return self._agent_name
    
    @property
    def model_name(self) -> str:
        """Get model name with caching."""
        if self._model_name is None:
            if hasattr(self.instance, 'model') and hasattr(self.instance.model, 'model_name'):
                self._model_name = str(self.instance.model.model_name)
            else:
                self._model_name = "unknown"
        return self._model_name

def set_span_attributes(span, operation_name: str, ctx: FrameworkInstrumentationContext, 
                       lifecycle_phase: Optional[str] = None,
                       additional_attrs: Optional[Dict[str, Any]] = None):
    """Optimized attribute setting with context caching."""
    
    # Set core attributes using cached context
    span.set_attribute(SemanticConvention.GEN_AI_OPERATION, operation_name)
    span.set_attribute(SemanticConvention.GEN_AI_SYSTEM, SemanticConvention.GEN_AI_SYSTEM_FRAMEWORK)
    span.set_attribute(SemanticConvention.GEN_AI_AGENT_NAME, ctx.agent_name)
    span.set_attribute(SemanticConvention.GEN_AI_REQUEST_MODEL, ctx.model_name)
    
    # Set environment attributes
    span.set_attribute(DEPLOYMENT_ENVIRONMENT, ctx.environment)
    span.set_attribute(SERVICE_NAME, ctx.application_name)
    span.set_attribute(SemanticConvention.GEN_AI_SDK_VERSION, ctx.version)
    
    # Set lifecycle phase if provided
    if lifecycle_phase:
        span.set_attribute(SemanticConvention.GEN_AI_AGENT_LIFECYCLE_PHASE, lifecycle_phase)
    
    # Set additional attributes
    if additional_attrs:
        for key, value in additional_attrs.items():
            span.set_attribute(key, value)
```

### Step 6.2: Performance Validation

**Measure instrumentation overhead:**
```python
def benchmark_performance():
    """Measure instrumentation performance impact"""
    import time
    
    # Test without instrumentation
    start = time.time()
    run_framework_operations(count=100)
    baseline = time.time() - start
    
    # Test with instrumentation
    openlit.init()
    start = time.time()
    run_framework_operations(count=100)
    instrumented = time.time() - start
    
    overhead = ((instrumented - baseline) / baseline) * 100
    print(f"Instrumentation overhead: {overhead:.2f}%")
    assert overhead < 10, "Overhead should be under 10%"
```

### Step 6.2: Business Intelligence Validation

**Verify OpenLIT's competitive advantages:**
```python
def test_business_intelligence():
    """Validate comprehensive business metrics capture"""
    spans = run_instrumentation_test()
    
    business_attributes = [
        'gen_ai.usage.input_tokens',
        'gen_ai.usage.output_tokens',
        'gen_ai.client.operation.duration',
        'gen_ai.request.model'
    ]
    
    for span in spans:
        attrs = span['attributes']
        found_attrs = [attr for attr in business_attributes if attr in attrs]
        print(f"Span '{span['name']}': {len(found_attrs)}/{len(business_attributes)} business attributes")
```

## Phase 7: Post-Change Cleanup

**CRITICAL**: Always run comprehensive cleanup after implementation:

### Step 7.1: Automated Code Quality Check

```bash
#!/bin/bash
# cleanup_instrumentation.sh
FRAMEWORK_DIR="src/openlit/instrumentation/$1"

echo "üßπ Cleaning up $FRAMEWORK_DIR..."

# Remove trailing whitespace
find "$FRAMEWORK_DIR" -name "*.py" -exec sed -i '' 's/[[:space:]]*$//' {} \;

# Add missing final newlines
find "$FRAMEWORK_DIR" -name "*.py" | while read file; do
    if [[ ! -s "$file" || $(tail -c1 "$file" | wc -l) -eq 0 ]]; then
        echo "" >> "$file"
    fi
done

# Check syntax
for file in "$FRAMEWORK_DIR"/*.py; do
    python3 -m py_compile "$file" || echo "‚ùå Syntax error in $file"
done

# Check line lengths
long_lines=$(find "$FRAMEWORK_DIR" -name "*.py" -exec awk 'length($0) > 80' {} \; | wc -l)
echo "Lines over 80 characters: $long_lines"
```

### Step 7.2: Comprehensive Pylint Error Handling

**Run Pylint Check with Project Configuration:**
```bash
# Use the project's pylintrc configuration
cd sdk/python
pylint src/openlit/instrumentation/{framework}/ --rcfile=.pylintrc

# Target scores:
# - Individual files: 9.5-10.0/10
# - Module overall: 9.0-9.5/10
# - Perfect 10.0/10 is achievable by ignoring import errors
```

**Note**: The project's `.pylintrc` disables many common warnings like `broad-exception-caught`, `too-many-locals`, etc.

### Step 7.2b: Pylint Optimization Patterns

**Common High-Impact Fixes:**

```python
# 1. Lazy logging (prevents W1203: logging-fstring-interpolation)
# ‚ùå BAD:
logger.debug(f"Failed to extract messages: {e}")

# ‚úÖ GOOD:
logger.debug("Failed to extract messages: %s", e)

# 2. Remove unused imports and variables
# ‚ùå BAD:
import importlib.metadata
from typing import Dict, Any, Optional, List, Union, Tuple

def wrapper(wrapped, instance, args, kwargs):
    method_name = wrapped.__name__  # Unused variable
    return wrapped(*args, **kwargs)

# ‚úÖ GOOD:
import json
from typing import Dict, Any, Optional, List, Tuple

def wrapper(wrapped, instance, args, kwargs):
    return wrapped(*args, **kwargs)

# 3. Fix exception handling
# ‚ùå BAD:
try:
    # some code
except Exception as e:  # Unused variable
    pass

# ‚úÖ GOOD:
try:
    # some code
except Exception:
    pass

# 4. Add class methods for R0903: Too few public methods
# ‚ùå BAD:
class CreateContext:
    def __init__(self):
        self.data = {}

# ‚úÖ GOOD:
class CreateContext:
    """Context for agent creation instrumentation."""
    def __init__(self):
        self.data = {}
    
    def get_context_info(self):
        """Get context information."""
        return self.data
    
    def has_data(self):
        """Check if context has data."""
        return bool(self.data)
```

### Step 7.3: Configure Project Pylint Rules

**Add `import-outside-toplevel` to disabled rules in `sdk/python/.pylintrc`:**

```ini
[MESSAGES CONTROL]
disable=
    # ... existing rules ...
    import-outside-toplevel    # Essential for optional dependencies
```

**Why**: Import-outside-toplevel warnings are triggered by correct optional dependency patterns in OpenLIT.

### Step 7.4: Optimize Dummy Classes for Perfect Score

**Eliminate unnecessary dummy classes to achieve 10.00/10:**

#### **Use TYPE_CHECKING Pattern for Type-Only Classes**
```python
from typing import TYPE_CHECKING

try:
    from framework import MainClass
    if TYPE_CHECKING:
        from framework import TypeOnlyClass1, TypeOnlyClass2
    AVAILABLE = True
except ImportError:
    class MainClass:
        """Dummy class - actually used at runtime"""
        def method(self): return None
    
    if TYPE_CHECKING:
        # Type hints only - don't exist at runtime
        TypeOnlyClass1 = Any
        TypeOnlyClass2 = Any
    AVAILABLE = False

# Use quoted type hints for methods
def process(self, item: "TypeOnlyClass1") -> None:
```

#### **When R0903 is Still Acceptable**
- **Status**: ‚úÖ **ACCEPTABLE** for classes actually used at runtime
- **Example**: TracingProcessor base class that's inherited from

**Target Score**: **10.00/10** is achievable for most instrumentations

### Step 7.5: Common Pylint Issues and Fixes

#### **C0301: Line Too Long**
**Problem**: Lines exceed character limit (135 characters per project pylintrc)

**Fix Pattern**:
```python
# BAD: Long function call
span.set_attribute(SemanticConvention.GEN_AI_REQUEST_FREQUENCY_PENALTY, handle_not_given(kwargs.get("frequency_penalty"), 0.0))

# GOOD: Split across multiple lines
span.set_attribute(
    SemanticConvention.GEN_AI_REQUEST_FREQUENCY_PENALTY, 
    handle_not_given(kwargs.get("frequency_penalty"), 0.0)
)

# BAD: Long function call with many parameters
common_span_attributes(scope, SemanticConvention.GEN_AI_OPERATION_TYPE_CHAT, SemanticConvention.GEN_AI_SYSTEM_OPENAI, server_address, server_port, request_model, response_model, environment, application_name, is_stream, tbt, ttft, version)

# GOOD: Split parameters logically
common_span_attributes(
    scope,
    SemanticConvention.GEN_AI_OPERATION_TYPE_CHAT, 
    SemanticConvention.GEN_AI_SYSTEM_OPENAI,
    server_address, server_port, request_model, 
    response_model, environment, application_name, 
    is_stream, tbt, ttft, version
)

# BAD: Long import
from openlit.instrumentation.framework.async_framework import async_general_wrap

# GOOD: Split import
from openlit.instrumentation.framework.async_framework import (
    async_general_wrap
)
```

#### **C0303: Trailing Whitespace**
**Automated Fix**:
```bash
# Remove all trailing whitespace
find src/openlit/instrumentation/{framework} -name "*.py" -exec sed -i '' 's/[[:space:]]*$//' {} \;
```

#### **C0304: Missing Final Newline**
**Automated Fix**:
```bash
# Add missing final newlines
find src/openlit/instrumentation/{framework} -name "*.py" | while read file; do
    if [[ ! -s "$file" || $(tail -c1 "$file" | wc -l) -eq 0 ]]; then
        echo "" >> "$file"
    fi
done
```

#### **C0115: Missing Class Docstring**
**Fix Pattern**:
```python
# BAD: No docstring
class TracingProcessor:
    def force_flush(self): pass

# GOOD: Add descriptive docstring
class TracingProcessor:
    """Dummy TracingProcessor class for when agents is not available"""
    
    def force_flush(self):
        """Dummy force_flush method"""
        pass
```

#### **C0116: Missing Function Docstring**
**Fix Pattern**:
```python
# BAD: No docstring
def _extract_model_info(self, data):
    return data.model

# GOOD: Add descriptive docstring
def _extract_model_info(self, data):
    """Extract model information from span data or agent configuration"""
    return data.model
```

#### **C0321: Multiple Statements on Single Line**
**Fix Pattern**:
```python
# BAD: Multiple statements
def force_flush(self): pass
class Trace: pass

# GOOD: Separate lines with proper formatting
def force_flush(self):
    """Dummy force_flush method"""
    pass

class Trace:
    """Dummy Trace class for when agents is not available"""
    pass
```

#### **C0415: Import Outside Toplevel**
**Context**: These warnings are **ACCEPTABLE** and **REQUIRED** for optional dependencies

**Correct Pattern (DO NOT "FIX")**:
```python
# This is CORRECT for optional dependencies - keep as-is
def _instrument(self, **kwargs):
    try:
        from agents import set_trace_processors  # CORRECT inside function
        set_trace_processors([processor])
    except ImportError:
        pass  # Package not available
```

**Why This is Correct**:
- Prevents import errors when target package isn't installed
- Essential for OpenLIT's optional dependency pattern
- Moving to top level would break instrumentation

**Only Fix When**:
```python
# ONLY move to top level for always-needed imports
import json  # At top of file when always used
from typing import Any, Dict, Optional

def _capture_model_parameters(self, span, data):
    try:
        params = {"model": data.model}
        span.set_attribute("gen_ai.request.parameters", json.dumps(params))
    except Exception:
        pass
```

#### **E0602: Undefined Variable**
**Problem**: Variable used before definition

**Fix Pattern**:
```python
# BAD: Undefined variable
cost = get_chat_model_cost(model, pricing_info, input_tokens, output_tokens)

# GOOD: Import the function
from openlit.__helpers import (
    common_framework_span_attributes,
    handle_exception,
    get_chat_model_cost  # Add missing import
)
```

#### **Syntax Errors: Missing except/finally**
**Problem**: Try block without proper exception handling

**Fix Pattern**:
```python
# BAD: Missing except block
def _extract_token_usage(self, span, data):
    try:
        usage = data.usage
        span.set_attribute("gen_ai.usage.input_tokens", usage.input_tokens)

# GOOD: Add except block
def _extract_token_usage(self, span, data):
    try:
        usage = data.usage
        span.set_attribute("gen_ai.usage.input_tokens", usage.input_tokens)
    except Exception:
        pass  # Ignore errors in token usage extraction
```

#### **R1702: Too Many Nested Blocks**
**Fix Pattern**:
```python
# BAD: Deep nesting
def process_data(self, data):
    if hasattr(data, 'config'):
        if data.config:
            if hasattr(data.config, 'model'):
                if data.config.model:
                    if isinstance(data.config.model, str):
                        return data.config.model

# GOOD: Early returns
def process_data(self, data):
    if not hasattr(data, 'config'):
        return None
    if not data.config:
        return None
    if not hasattr(data.config, 'model'):
        return None
    if not data.config.model:
        return None
    if not isinstance(data.config.model, str):
        return None
    return data.config.model
```

#### **R0903: Too Few Public Methods**
**Context**: **ACCEPTABLE** and **EXPECTED** for dummy/placeholder classes

**Correct Pattern (DO NOT "FIX")**:
```python
# This is CORRECT for dummy classes - keep as-is
class TracingProcessor:
    """Dummy TracingProcessor class for when agents is not available"""
    
    def force_flush(self):
        """Dummy force_flush method"""
        return None
    
    def shutdown(self):
        """Dummy shutdown method"""
        return None
```

**Why This is Correct**:
- Dummy classes by design to prevent import errors
- Only need minimal methods to satisfy interfaces
- Adding unnecessary methods would be incorrect

### Step 7.6: Automated Pylint Fix Script

**Create comprehensive fix script:**
```bash
#!/bin/bash
# fix_pylint_issues.sh
FRAMEWORK_DIR="src/openlit/instrumentation/$1"

echo "üîß Fixing Pylint issues in $FRAMEWORK_DIR..."

# 1. Remove trailing whitespace
echo "  ‚úÇÔ∏è  Removing trailing whitespace..."
find "$FRAMEWORK_DIR" -name "*.py" -exec sed -i '' 's/[[:space:]]*$//' {} \;

# 2. Add missing final newlines
echo "  üìù Adding missing final newlines..."
find "$FRAMEWORK_DIR" -name "*.py" | while read file; do
    if [[ ! -s "$file" || $(tail -c1 "$file" | wc -l) -eq 0 ]]; then
        echo "" >> "$file"
    fi
done

# 3. Check syntax errors
echo "  üîç Checking syntax..."
syntax_errors=0
for file in "$FRAMEWORK_DIR"/*.py; do
    if ! python3 -m py_compile "$file" 2>/dev/null; then
        echo "    ‚ùå Syntax error in $file"
        syntax_errors=$((syntax_errors + 1))
    fi
done

# 4. Check line lengths and provide guidance
echo "  üìè Checking line lengths..."
long_lines=$(find "$FRAMEWORK_DIR" -name "*.py" -exec awk 'length($0) > 135 { print FILENAME ":" NR ":" length($0) ":" $0 }' {} \;)
if [ -n "$long_lines" ]; then
    echo "    ‚ö†Ô∏è  Long lines found (>135 chars):"
    echo "$long_lines" | head -5
    echo "    üí° Split long lines using the patterns in the guide"
fi

# 5. Final validation
echo "  ‚úÖ Running final validation..."
if [ $syntax_errors -eq 0 ]; then
    echo "‚úÖ All syntax errors fixed!"
else
    echo "‚ùå $syntax_errors syntax errors remaining - manual fix required"
fi

# 6. Run limited pylint check
echo "  üîç Running key pylint checks..."
pylint --disable=all --enable=syntax-error,undefined-variable,trailing-whitespace,missing-final-newline "$FRAMEWORK_DIR" 2>/dev/null || echo "Some pylint issues remain - check manually"

echo "üéâ Pylint fix script completed!"
echo "üí° Run full pylint check and fix remaining issues manually"
```

### Step 7.7: Manual Fix Checklist

**Before committing, verify:**
- [ ] ‚úÖ All files compile without syntax errors
- [ ] ‚úÖ No trailing whitespace
- [ ] ‚úÖ All files end with newline  
- [ ] ‚úÖ No lines over 135 characters (or split appropriately)
- [ ] ‚úÖ All classes have docstrings
- [ ] ‚úÖ All public methods have docstrings
- [ ] ‚úÖ No undefined variables
- [ ] ‚úÖ Try blocks have except/finally
- [ ] ‚úÖ Imports are at top level when possible
- [ ] ‚úÖ Nested blocks reduced where possible

### Step 7.8: Usage Instructions

**Run the fix script:**
```bash
chmod +x fix_pylint_issues.sh
./fix_pylint_issues.sh openai_agents
./fix_pylint_issues.sh haystack
```

**Final validation:**
```bash
# Check specific framework
pylint src/openlit/instrumentation/openai_agents/

# Quick syntax check
python3 -m py_compile src/openlit/instrumentation/openai_agents/*.py

# Test instrumentation still works
cd sdk/python && PYTHONPATH=src python -c "
import openlit
openlit.init()
print('‚úÖ Instrumentation works after pylint fixes')
"
```

### Step 7.9: Testing After Pylint Fixes

**CRITICAL**: Always test that instrumentation works after fixing pylint issues:

```python
# test_after_pylint_fix.py
import sys
sys.path.insert(0, 'sdk/python/src')

import openlit
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

class TestExporter:
    def __init__(self): 
        self.spans = []
    def export(self, spans): 
        self.spans.extend(spans)
        return 0
    def shutdown(self): 
        pass

def test_instrumentation_after_fixes():
    """Test that instrumentation works after pylint fixes"""
    exporter = TestExporter()
    tracer_provider = TracerProvider()
    tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))
    
    # Initialize OpenLIT
    openlit.init(detailed_tracing=True)
    
    # Run basic test based on framework
    # ... framework-specific test code ...
    
    tracer_provider.force_flush(1000)
    
    assert len(exporter.spans) > 0, "Instrumentation should generate spans"
    print(f"‚úÖ Generated {len(exporter.spans)} spans after pylint fixes")
    
    # Validate span naming convention
    for span in exporter.spans:
        assert ' ' in span.name, f"Span '{span.name}' should follow '{{operation_type}} {{operation_name}}' format"
    
    print("‚úÖ All tests passed - instrumentation working correctly")

if __name__ == "__main__":
    test_instrumentation_after_fixes()
```

## Code Standards & Patterns

### Span Naming Convention

**MANDATORY Format**: `{operation_type} {operation_name}`

```python
# ‚úÖ CORRECT
"agent Triage agent"
"chat gpt-4o"
"retrieve documents"
"generate response"
"workflow multi-agent"

# ‚ùå INCORRECT  
"Triage agent"           # Missing operation type
"agent_execution"        # Wrong format
"chat_completion_gpt4"   # Wrong format
```

### Semantic Conventions Usage

**Extensive usage required - add to semcov.py when needed:**

```python
# Always use semantic conventions
span.set_attribute(SemanticConvention.GEN_AI_SYSTEM, "framework_name")
span.set_attribute(SemanticConvention.GEN_AI_OPERATION_NAME, operation_name)
span.set_attribute(SemanticConvention.GEN_AI_REQUEST_MODEL, model)

# Add new conventions when needed
class SemanticConvention:
    GEN_AI_AGENT_DESCRIPTION = "gen_ai.agent.description"
    GEN_AI_WORKFLOW_TYPE = "gen_ai.workflow.type"
```

### Business Intelligence Attributes

**Always capture OpenLIT's competitive advantages:**

```python
# Token usage and cost tracking
span.set_attribute(SemanticConvention.GEN_AI_USAGE_INPUT_TOKENS, input_tokens)
span.set_attribute(SemanticConvention.GEN_AI_USAGE_OUTPUT_TOKENS, output_tokens)
span.set_attribute(SemanticConvention.GEN_AI_CLIENT_OPERATION_DURATION, duration)

# Content capture with MIME types
span.set_attribute(SemanticConvention.GEN_AI_CONTENT_PROMPT, content)
span.set_attribute(SemanticConvention.GEN_AI_CONTENT_COMPLETION, response)
```

## Quality Checklist

### Phase Completion Checklist

**Phase 1 - Competitive Analysis:**
- [ ] Competitor repositories cloned and analyzed
- [ ] Integration patterns documented
- [ ] Competitive gaps identified
- [ ] OpenLIT advantages planned

**Phase 2 - Framework Analysis:**
- [ ] Built-in capabilities explored
- [ ] Method discovery completed
- [ ] Integration pattern chosen
- [ ] Operation mapping defined

**Phase 3 - Implementation Strategy:**
- [ ] Span naming convention defined
- [ ] Semantic conventions planned
- [ ] Test infrastructure designed

**Phase 4 - Implementation:**
- [ ] Test script created first
- [ ] Integration pattern implemented
- [ ] Semantic conventions used extensively
- [ ] Business intelligence captured

**Phase 5 - Testing:**
- [ ] Span hierarchy validated
- [ ] Competitive comparison passed
- [ ] Mock testing completed
- [ ] Error resilience tested

**Phase 6 - Optimization:**
- [ ] Performance overhead measured (<10%)
- [ ] Business intelligence validated
- [ ] Competitive advantages confirmed

**Phase 7 - Cleanup:**
- [ ] Code quality checks passed
- [ ] Syntax errors resolved
- [ ] Line length compliant
- [ ] Final validation completed

### Success Criteria

**Instrumentation is complete when:**
1. ‚úÖ **Competitive Analysis**: Studied OpenInference, OpenLLMetry, AgentOps, LangFuse
2. ‚úÖ **Target SDK Study**: Cloned and analyzed framework's internal structure
3. ‚úÖ **Semantic Conventions**: Uses SemanticConvention extensively (no hardcoded strings)
4. ‚úÖ **Span Hierarchy**: Generates more/better spans than competitors
5. ‚úÖ **Naming Convention**: Follows `{operation_type} {operation_name}` format
6. ‚úÖ **Context Caching**: Implements performance optimization patterns
7. ‚úÖ **Business Intelligence**: Captures comprehensive cost/token/performance data
8. ‚úÖ **Performance**: Overhead <10% with benchmarking vs competitors
9. ‚úÖ **Error Resilience**: Handles framework version differences gracefully
10. ‚úÖ **Code Quality**: Achieves 9.5-10.0/10 pylint score

**OpenLIT Competitive Advantages Delivered:**
- üéØ **Superior Business Intelligence**: Detailed cost tracking, token usage, performance metrics
- üîó **Cross-System Integration**: Links to LLM provider spans with proper hierarchy
- üé® **Enhanced Observability**: MIME types, structured content capture
- üõ°Ô∏è **Error Resilience**: Graceful degradation across framework versions
- üìä **Comprehensive Coverage**: More spans and attributes than all competitors
- ‚ö° **Performance Leadership**: Context caching and optimization patterns
- üèóÔ∏è **Flexible Architecture**: Adapts to different framework patterns (processor, callback, wrapper) 