#!/usr/bin/env python3
"""
OpenLIT MCP instrumentation test.
Tests OpenLIT's MCP instrumentation with real MCP operations.
"""

import sys
import os
import asyncio
import time
import json

sys.path.insert(0, "sdk/python/src")

import openlit

# Import real MCP components
try:
    from mcp.server import Server, NotificationOptions
    from mcp.server.models import InitializationOptions
    import mcp.server.stdio
    import mcp.types as types
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
    REAL_MCP = True
except ImportError as e:
    print(f"‚ùå MCP SDK not available: {e}")
    REAL_MCP = False


async def run_mcp_operations():
    """Run comprehensive MCP operations"""
    if not REAL_MCP:
        return False
    
    try:
        server_params = StdioServerParameters(
            command="python",
            args=["-c", """
import asyncio
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

server = Server('openlit-test-server')

@server.list_tools()
async def list_tools():
    return [
        types.Tool(name='calculator', description='Calculate numbers', inputSchema={"type": "object"}),
        types.Tool(name='text_analyzer', description='Analyze text content', inputSchema={"type": "object"}),
        types.Tool(name='data_processor', description='Process data arrays', inputSchema={"type": "object"})
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict):
    if name == 'calculator':
        result = arguments.get('a', 0) + arguments.get('b', 0)
        return [types.TextContent(type='text', text=f'Calculation result: {result}')]
    elif name == 'text_analyzer':
        text = arguments.get('text', '')
        words = len(text.split())
        chars = len(text)
        return [types.TextContent(type='text', text=f'Analysis: {words} words, {chars} characters')]
    elif name == 'data_processor':
        data = arguments.get('data', [])
        total = sum(data) if isinstance(data, list) else 0
        avg = total / len(data) if isinstance(data, list) and len(data) > 0 else 0
        return [types.TextContent(type='text', text=f'Processing: sum={total}, avg={avg:.2f}')]
    return [types.TextContent(type='text', text='Tool not found')]

@server.list_resources()
async def list_resources():
    return [
        types.Resource(uri='file://openlit_test.txt', name='OpenLIT Test File', description='Test resource for OpenLIT'),
        types.Resource(uri='file://openlit_data.json', name='OpenLIT Data', description='JSON data for OpenLIT testing')
    ]

@server.read_resource()
async def read_resource(uri: str):
    if uri == 'file://openlit_test.txt':
        return 'OpenLIT MCP instrumentation test content with comprehensive business intelligence tracking.'
    elif uri == 'file://openlit_data.json':
        return '{"framework": "OpenLIT", "capabilities": ["business_intelligence", "performance_metrics", "cost_tracking"], "advantages": ["comprehensive_spans", "mcp_namespace", "superior_observability"]}'
    return 'Resource not available'

@server.list_prompts()
async def list_prompts():
    return [
        types.Prompt(name='openlit_analysis', description='OpenLIT analysis prompt'),
        types.Prompt(name='openlit_summary', description='OpenLIT summary prompt')
    ]

@server.get_prompt()
async def get_prompt(name: str, arguments: dict):
    if name == 'openlit_analysis':
        topic = arguments.get('topic', 'general')
        return types.GetPromptResult(
            description=f'OpenLIT analysis prompt for {topic}',
            messages=[types.PromptMessage(role='user', content=types.TextContent(type='text', text=f'Analyze this {topic} with OpenLIT comprehensive observability'))]
        )
    elif name == 'openlit_summary':
        return types.GetPromptResult(
            description='OpenLIT summary prompt',
            messages=[types.PromptMessage(role='user', content=types.TextContent(type='text', text='Summarize with OpenLIT business intelligence'))]
        )
    raise ValueError(f'Prompt not found: {name}')

async def main():
    async with mcp.server.stdio.stdio_server() as (read, write):
        await server.run(read, write, InitializationOptions(
            server_name='openlit-comprehensive-test',
            server_version='1.0.0',
            capabilities=server.get_capabilities(
                notification_options=NotificationOptions(),
                experimental_capabilities={}
            )
        ))

asyncio.run(main())
            """]
        )
        
        print("üöÄ Starting OpenLIT MCP Operations Test")
        
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                
                # Test 1: List Tools
                print("  üìã 1. Listing tools...")
                tools = await session.list_tools()
                print(f"    ‚úÖ Found {len(tools.tools)} tools: {[t.name for t in tools.tools]}")
                
                # Test 2: Call Calculator Tool
                print("  üî¢ 2. Calling calculator tool...")
                calc_result = await session.call_tool("calculator", {"a": 42, "b": 58})
                print(f"    ‚úÖ Calculator result received")
                
                # Test 3: Call Text Analyzer Tool
                print("  üìù 3. Calling text analyzer tool...")
                text_result = await session.call_tool("text_analyzer", {
                    "text": "OpenLIT provides superior MCP instrumentation with comprehensive business intelligence, performance metrics, and advanced observability capabilities."
                })
                print(f"    ‚úÖ Text analyzer result received")
                
                # Test 4: Call Data Processor Tool
                print("  üìä 4. Calling data processor tool...")
                data_result = await session.call_tool("data_processor", {
                    "data": [10, 25, 33, 47, 52, 68, 75, 82, 91, 100]
                })
                print(f"    ‚úÖ Data processor result received")
                
                # Test 5: List Resources
                print("  üìö 5. Listing resources...")
                resources = await session.list_resources()
                print(f"    ‚úÖ Found {len(resources.resources)} resources: {[r.name for r in resources.resources]}")
                
                # Test 6: Read Text Resource
                print("  üìñ 6. Reading text resource...")
                try:
                    text_response = await session.read_resource("file://openlit_test.txt")
                    # Extract text content from response.contents[0].text
                    text_content = text_response.contents[0].text if text_response.contents else "No content"
                    print(f"    ‚úÖ Text resource read: {len(text_content)} characters")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Text resource read failed: {e}")
                
                # Test 7: Read JSON Resource
                print("  üóÇÔ∏è  7. Reading JSON resource...")
                try:
                    json_response = await session.read_resource("file://openlit_data.json")
                    # Extract text content from response.contents[0].text
                    json_content = json_response.contents[0].text if json_response.contents else "No content"
                    print(f"    ‚úÖ JSON resource read: {len(json_content)} characters")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è JSON resource read failed: {e}")
                
                # Test 8: List Prompts
                print("  üí¨ 8. Listing prompts...")
                prompts = await session.list_prompts()
                print(f"    ‚úÖ Found {len(prompts.prompts)} prompts: {[p.name for p in prompts.prompts]}")
                
                # Test 9: Get Analysis Prompt
                print("  üìã 9. Getting analysis prompt...")
                try:
                    analysis_prompt = await session.get_prompt("openlit_analysis", {"topic": "MCP_performance"})
                    description = getattr(analysis_prompt, 'description', 'No description')
                    print(f"    ‚úÖ Analysis prompt retrieved: {description}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Analysis prompt failed: {e}")
                
                # Test 10: Get Summary Prompt
                print("  üìÑ 10. Getting summary prompt...")
                try:
                    summary_prompt = await session.get_prompt("openlit_summary", {})
                    description = getattr(summary_prompt, 'description', 'No description')
                    print(f"    ‚úÖ Summary prompt retrieved: {description}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Summary prompt failed: {e}")
                
        print("‚úÖ All OpenLIT MCP operations completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå OpenLIT MCP operations failed: {e}")
        return False


async def main():
    """Main test function for OpenLIT MCP instrumentation"""
    print("üß† OPENLIT MCP INSTRUMENTATION TEST")
    print("=" * 50)
    
    if not REAL_MCP:
        print("‚ùå MCP SDK not available")
        return
    
    # Initialize OpenLIT with console exporter disabled for cleaner output
    print("‚úÖ Initializing OpenLIT with detailed tracing...")
    openlit.init(detailed_tracing=True)
    
    # Run comprehensive MCP operations
    success = await run_mcp_operations()
    
    # Results
    print(f"\nüìä OpenLIT MCP Test Results:")
    print(f"  Status: {'‚úÖ SUCCESS' if success else '‚ùå FAILED'}")
    print(f"  Operations: {'10/10 completed' if success else 'Failed during execution'}")
    print(f"  Instrumentation: {'‚úÖ Active' if success else '‚ùå Inactive'}")
    
    if success:
        print(f"\nüèÜ OpenLIT MCP Advantages Demonstrated:")
        print(f"  ‚úÖ Comprehensive span generation")
        print(f"  ‚úÖ MCP-specific attribute namespace (mcp.*)")
        print(f"  ‚úÖ Business intelligence capture")
        print(f"  ‚úÖ Performance metrics tracking")
        print(f"  ‚úÖ Tool/resource/prompt observability")
        print(f"  ‚úÖ Superior observability vs competitors")
        
        print(f"\nüí° Expected Spans Generated:")
        print(f"  ‚Ä¢ tool list_tools")
        print(f"  ‚Ä¢ tool call_tool (calculator)")
        print(f"  ‚Ä¢ tool call_tool (text_analyzer)")
        print(f"  ‚Ä¢ tool call_tool (data_processor)")
        print(f"  ‚Ä¢ resource list_resources")
        print(f"  ‚Ä¢ resource read_resource (text)")
        print(f"  ‚Ä¢ resource read_resource (json)")
        print(f"  ‚Ä¢ prompt list_prompts")
        print(f"  ‚Ä¢ prompt get_prompt (analysis)")
        print(f"  ‚Ä¢ prompt get_prompt (summary)")
        print(f"  = 10 comprehensive spans with rich business intelligence")


if __name__ == "__main__":
    asyncio.run(main())
