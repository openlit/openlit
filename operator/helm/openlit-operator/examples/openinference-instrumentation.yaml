# OpenInference Provider Example
# This example shows how to use the OpenInference instrumentation provider
# which provides comprehensive AI/ML framework instrumentation

apiVersion: openlit.io/v1alpha1
kind: AutoInstrumentation
metadata:
  name: openinference-instrumentation
  namespace: ai-workloads
spec:
  # Select pods using OpenInference
  selector:
    matchLabels:
      instrumentation.provider: "openinference"
      app.category: "ai-ml"
  
  # OTLP endpoint (OpenInference works with OpenLIT backend)
  otlp:
    endpoint: "http://openlit.openlit.svc.cluster.local:4318"
    timeout: 30
  
  # OpenInference Python instrumentation
  python:
    instrumentation:
      enabled: true
      provider: openinference  # Use OpenInference provider
      version: latest
      
      # Additional packages specific to your AI/ML stack
      customPackages: "langchain>=0.1.0,llama-index>=0.9.0,openai>=1.0.0"
      
      # Environment variables for OpenInference configuration
      env:
      - name: OTEL_SERVICE_NAME
        value: "ai-chat-service"
      - name: OTEL_DEPLOYMENT_ENVIRONMENT
        value: "production"
      
      # API keys for AI services
      - name: OPENAI_API_KEY
        valueFrom:
          secretKeyRef:
            name: ai-api-keys
            key: openai
      - name: ANTHROPIC_API_KEY
        valueFrom:
          secretKeyRef:
            name: ai-api-keys
            key: anthropic
            optional: true
  
  # Resource configuration
  resource:
    environment: production
